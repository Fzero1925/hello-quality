#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹ç›¸ä¼¼åº¦æ£€æµ‹å·¥å…·
ä¸“é—¨ç”¨äºæ£€æµ‹æ–‡ç« ç›¸ä¼¼åº¦ï¼Œå¤„ç†é‡å¤å†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š

åŠŸèƒ½ç‰¹ç‚¹ï¼š
- æ‰¹é‡æ£€æµ‹æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ–‡ç« çš„ç›¸ä¼¼åº¦
- è‡ªåŠ¨å¤„ç†é‡å¤æ–‡ç« ï¼ˆä¿ç•™æœ€æ–°ï¼Œç§»åŠ¨å…¶ä»–åˆ°duplicate_articlesæ–‡ä»¶å¤¹ï¼‰
- ç”Ÿæˆè¯¦ç»†çš„ç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š
- æ”¯æŒé…ç½®ç›¸ä¼¼åº¦é˜ˆå€¼å’Œæ£€æµ‹å‚æ•°
"""

import os
import sys
import argparse
import yaml
import json
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import re
import hashlib
import codecs

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class SimilarityChecker:
    """ç‹¬ç«‹ç›¸ä¼¼åº¦æ£€æµ‹å™¨"""

    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–ç›¸ä¼¼åº¦æ£€æµ‹å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.similarity_threshold = self.config.get('tfidf_threshold', 0.5)
        self.comparison_window_days = self.config.get('comparison_window_days', 7)
        self.new_articles_folder = self.config.get('new_articles_folder', 'new-articles')
        self.old_articles_folder = self.config.get('old_articles_folder', 'old-articles')
        self.keep_oldest = self.config.get('keep_oldest_article', True)

        # æ£€æµ‹ç»“æœå­˜å‚¨
        self.all_articles = []

        # ä¸»é¢˜åˆ†ç±»é…ç½® (ä»é…ç½®æ–‡ä»¶è¯»å–)
        self.topic_classification = self.config.get('topic_classification', {})
        self.cross_topic_threshold = self.topic_classification.get('cross_topic_similarity_threshold', 0.85)

        # è°ƒè¯•æ¨¡å¼
        self.debug_mode = False

        print(f"âœ… ç›¸ä¼¼åº¦æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š æ£€æµ‹é˜ˆå€¼: {self.similarity_threshold}")
        print(f"â° æ£€æµ‹çª—å£: {self.comparison_window_days} å¤©")

    def _load_config(self, config_path: Optional[str] = None) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'similarity_threshold': 0.7,
            'comparison_window_days': 90,
            'duplicate_folder': 'oldarticles',
            'min_content_length': 1000,
            'check_title_similarity': True,
            'check_content_similarity': True,
            'title_weight': 0.3,
            'content_weight': 0.7,
            'preserve_newest': True,
            'backup_before_move': True,
            'topic_classification': {'enabled': False}
        }

        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f) or {}
                    similarity_config = user_config.get('similarity_detection', {})

                    # æ›´æ–°åŸºç¡€é…ç½®
                    if 'tfidf_threshold' in similarity_config:
                        default_config['similarity_threshold'] = similarity_config['tfidf_threshold']
                    if 'old_articles_folder' in similarity_config:
                        default_config['duplicate_folder'] = similarity_config['old_articles_folder']

                    # æ›´æ–°æ‰€æœ‰é…ç½®
                    default_config.update(similarity_config)

                    print(f"âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½è®¾ç½®: {config_path}")
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")

        return default_config

    def scan_articles(self, directory: str) -> List[Dict]:
        """æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ç« 

        Args:
            directory: è¦æ‰«æçš„ç›®å½•è·¯å¾„

        Returns:
            æ–‡ç« ä¿¡æ¯åˆ—è¡¨
        """
        directory_path = Path(directory)
        if not directory_path.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
            return []

        # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
        md_files = list(directory_path.glob("*.md"))
        if not md_files:
            print(f"âŒ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°Markdownæ–‡ä»¶: {directory}")
            return []

        print(f"ğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡ç« æ–‡ä»¶")

        articles = []
        for i, file_path in enumerate(md_files, 1):
            print(f"  [{i}/{len(md_files)}] æ­£åœ¨åˆ†æ: {file_path.name}")

            try:
                article_info = self._extract_article_info(file_path)
                if article_info:
                    articles.append(article_info)
            except Exception as e:
                print(f"    âš ï¸ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

        self.all_articles = articles
        print(f"âœ… æˆåŠŸåˆ†æ {len(articles)} ç¯‡æ–‡ç« ")
        return articles

    def _extract_article_info(self, file_path: Path) -> Optional[Dict]:
        """æå–æ–‡ç« ä¿¡æ¯

        Args:
            file_path: æ–‡ç« æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ç« ä¿¡æ¯å­—å…¸
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–Front Matterå’Œæ­£æ–‡
            front_matter, article_content = self._extract_front_matter(content)

            # è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
            file_stat = file_path.stat()

            # æå–åŸºæœ¬ä¿¡æ¯
            title = self._extract_title(front_matter, article_content, file_path.stem)

            article_info = {
                'file_path': str(file_path),
                'file_name': file_path.name,
                'file_stem': file_path.stem,
                'title': title,
                'content': article_content,
                'full_content': content,
                'word_count': len(article_content.split()),
                'char_count': len(article_content),
                'created_time': datetime.fromtimestamp(file_stat.st_ctime),
                'modified_time': datetime.fromtimestamp(file_stat.st_mtime),
                'file_size': file_stat.st_size,
                'content_hash': hashlib.md5(article_content.encode('utf-8')).hexdigest(),
                'title_hash': hashlib.md5(title.encode('utf-8')).hexdigest() if title else "",
                'front_matter': front_matter
            }

            return article_info

        except Exception as e:
            print(f"    âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return None

    def _extract_front_matter(self, content: str) -> Tuple[str, str]:
        """æå–Front Matterå’Œæ­£æ–‡"""
        if not content.strip().startswith('---'):
            return "", content

        lines = content.split('\n')
        end_index = -1
        for i, line in enumerate(lines[1:], 1):
            if line.strip() == '---':
                end_index = i
                break

        if end_index == -1:
            return "", content

        front_matter_text = '\n'.join(lines[1:end_index])
        article_body = '\n'.join(lines[end_index + 1:])

        return front_matter_text, article_body

    def _extract_title(self, front_matter: str, content: str, fallback: str) -> str:
        """æå–æ–‡ç« æ ‡é¢˜"""
        # å°è¯•ä»Front Matteræå–
        try:
            fm_data = yaml.safe_load(front_matter) or {}
            if 'title' in fm_data and fm_data['title']:
                return str(fm_data['title']).strip()
        except:
            pass

        # å°è¯•ä»å†…å®¹ä¸­æå–H1æ ‡é¢˜
        h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if h1_match:
            return h1_match.group(1).strip()

        # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºåå¤‡
        return fallback.replace('-', ' ').replace('_', ' ').title()

    def detect_similarities(self, articles: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """çº¿æ€§ç›¸ä¼¼åº¦æ£€æµ‹ç®—æ³•

        é€»è¾‘ï¼šæŒ‰æ—¶é—´æ’åºï¼Œæœ€æ—©æ–‡ç« vså…¶ä»–æ–‡ç« æ¯”è¾ƒ
        ç›¸ä¼¼çš„ç§»åŠ¨åˆ°old-articlesï¼Œä¸ç›¸ä¼¼çš„ä¿ç•™

        Args:
            articles: æ–‡ç« åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.all_articles

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        if articles is None:
            articles = self.all_articles

        if len(articles) < 2:
            print("ğŸ“Š æ–‡ç« æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼åº¦æ¯”è¾ƒ")
            return {'kept_articles': articles, 'moved_articles': [], 'total_comparisons': 0}

        print(f"ğŸ” å¼€å§‹çº¿æ€§ç›¸ä¼¼åº¦æ£€æµ‹ {len(articles)} ç¯‡æ–‡ç« ...")

        # 1. æŒ‰æœ‰æ•ˆæ—¥æœŸæ’åºï¼ˆæœ€æ—©åˆ°æœ€æ–°ï¼‰
        articles_sorted = sorted(articles, key=lambda x: self._get_article_effective_date(x))

        # ä¸ºæ–‡ç« æ·»åŠ æœ‰æ•ˆæ—¥æœŸ
        for article in articles_sorted:
            article['effective_date'] = self._get_article_effective_date(article)

        print(f"ğŸ“… æ–‡ç« æŒ‰æ—¥æœŸæ’åºå®Œæˆï¼Œæ—¶é—´èŒƒå›´: {articles_sorted[0]['effective_date'].strftime('%Y-%m-%d')} åˆ° {articles_sorted[-1]['effective_date'].strftime('%Y-%m-%d')}")

        # 2. çº¿æ€§æ¯”è¾ƒå¤„ç†
        kept_articles = []
        moved_articles = []
        total_comparisons = 0
        processing_date = datetime.now().strftime('%Y-%m-%d')

        remaining_articles = articles_sorted[:]  # åˆ›å»ºå‰¯æœ¬

        while remaining_articles:
            # å–æœ€æ—©çš„æ–‡ç« ä½œä¸ºåŸºå‡†
            base_article = remaining_articles.pop(0)
            kept_articles.append(base_article)

            print(f"\nğŸ“ å¤„ç†åŸºå‡†æ–‡ç« : {base_article['file_name']} (æ—¥æœŸ: {base_article['effective_date'].strftime('%Y-%m-%d')})")

            # ä¸å‰©ä½™æ–‡ç« æ¯”è¾ƒ
            to_remove = []
            for i, other_article in enumerate(remaining_articles):
                # æ£€æŸ¥æ—¶é—´çª—å£
                time_diff = abs((base_article['effective_date'] - other_article['effective_date']).days)
                if time_diff > self.comparison_window_days:
                    if self.debug_mode:
                        print(f"    ğŸ• è·³è¿‡ {other_article['file_name']} - æ—¶é—´å·® {time_diff} å¤© > {self.comparison_window_days} å¤©çª—å£")
                    continue

                # è·³è¿‡å†…å®¹å¤ªçŸ­çš„æ–‡ç« 
                if (base_article['word_count'] < self.config.get('min_content_length', 1000) or
                    other_article['word_count'] < self.config.get('min_content_length', 1000)):
                    if self.debug_mode:
                        print(f"    ğŸ“ è·³è¿‡ {other_article['file_name']} - å­—æ•°ä¸è¶³ (base: {base_article['word_count']}, other: {other_article['word_count']})")
                    continue

                total_comparisons += 1

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity_score = self._calculate_article_similarity(base_article, other_article)

                # æ£€æŸ¥æ˜¯å¦ä¸ºè·¨ä¸»é¢˜æ¯”è¾ƒ
                is_cross_topic = self._are_cross_topic_articles(base_article, other_article)
                effective_threshold = self.cross_topic_threshold if is_cross_topic else self.similarity_threshold

                if self.debug_mode:
                    title_sim = self._calculate_title_similarity(base_article['title'], other_article['title'])
                    content_sim = self._calculate_content_similarity(base_article['content'], other_article['content'])
                    topic1 = self._classify_article_topic(base_article)
                    topic2 = self._classify_article_topic(other_article)

                    print(f"    ğŸ” æ¯”è¾ƒ {other_article['file_name']}:")
                    print(f"      æ ‡é¢˜ç›¸ä¼¼åº¦: {title_sim:.3f}")
                    print(f"      å†…å®¹ç›¸ä¼¼åº¦: {content_sim:.3f}")
                    print(f"      ç»¼åˆç›¸ä¼¼åº¦: {similarity_score:.3f}")
                    print(f"      ä¸»é¢˜åˆ†ç±»: {topic1} vs {topic2}")
                    print(f"      è·¨ä¸»é¢˜: {is_cross_topic}, é˜ˆå€¼: {effective_threshold:.3f}")

                if similarity_score >= effective_threshold:
                    print(f"  ğŸ“¦ ç›¸ä¼¼æ–‡ç«  (ç›¸ä¼¼åº¦: {similarity_score:.3f}): {other_article['file_name']}")

                    # è®°å½•ç§»åŠ¨ä¿¡æ¯
                    moved_info = {
                        **other_article,
                        'similarity_to_base': similarity_score,
                        'base_article': base_article['file_name'],
                        'is_cross_topic': is_cross_topic,
                        'effective_threshold': effective_threshold
                    }
                    moved_articles.append(moved_info)
                    to_remove.append(i)
                elif self.debug_mode:
                    print(f"    âŒ ä¸ç›¸ä¼¼ {similarity_score:.3f} < {effective_threshold:.3f}")

            # ç§»é™¤å·²æ ‡è®°çš„æ–‡ç« 
            for index in reversed(to_remove):
                remaining_articles.pop(index)

        result = {
            'kept_articles': kept_articles,
            'moved_articles': moved_articles,
            'total_comparisons': total_comparisons,
            'processing_date': processing_date
        }

        print(f"\nâœ… çº¿æ€§æ£€æµ‹å®Œæˆ:")
        print(f"  ğŸ“Š æ€»æ¯”è¾ƒæ¬¡æ•°: {total_comparisons}")
        print(f"  âœ… ä¿ç•™æ–‡ç« : {len(kept_articles)} ç¯‡")
        print(f"  ğŸ“¦ ç§»åŠ¨æ–‡ç« : {len(moved_articles)} ç¯‡")

        return result

    def detect_duplicate_groups(self, articles: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """å…¨è¿æ¥å›¾ç›¸ä¼¼åº¦æ£€æµ‹ç®—æ³• - æ£€æµ‹æ‰€æœ‰é‡å¤æ–‡ç« ç¾¤ç»„

        ä½¿ç”¨å…¨è¿æ¥å›¾å’Œè¿é€šåˆ†é‡ç®—æ³•ï¼Œç¡®ä¿æ•°å­¦ä¸Šçš„å®Œæ•´æ€§ï¼Œä¸é—æ¼ä»»ä½•ç›¸ä¼¼å…³ç³»
        åªåˆ†æä¸ç§»åŠ¨æ–‡ä»¶ï¼Œç”Ÿæˆè¯¦ç»†çš„é‡å¤æ–‡ç« åˆ†ææŠ¥å‘Š

        Args:
            articles: æ–‡ç« åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.all_articles

        Returns:
            åŒ…å«é‡å¤ç¾¤ç»„åˆ†æçš„è¯¦ç»†ç»“æœå­—å…¸
        """
        if articles is None:
            articles = self.all_articles

        if len(articles) < 2:
            print("ğŸ“Š æ–‡ç« æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼åº¦æ¯”è¾ƒ")
            return {'duplicate_groups': [], 'unique_articles': articles, 'similarity_matrix': [], 'total_comparisons': 0}

        print(f"ğŸ” å¼€å§‹å…¨è¿æ¥å›¾ç›¸ä¼¼åº¦æ£€æµ‹ {len(articles)} ç¯‡æ–‡ç« ...")
        print("ğŸ“Š ç®—æ³•è¯´æ˜: è®¡ç®—æ‰€æœ‰æ–‡ç« å¯¹ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨è¿é€šåˆ†é‡ç®—æ³•è¯†åˆ«é‡å¤ç¾¤ç»„")

        # è¿‡æ»¤æ‰å­—æ•°ä¸è¶³çš„æ–‡ç« 
        valid_articles = []
        min_length = self.config.get('min_content_length', 1000)

        for article in articles:
            if article['word_count'] >= min_length:
                valid_articles.append(article)
            elif self.debug_mode:
                print(f"ğŸ“ è·³è¿‡å­—æ•°ä¸è¶³çš„æ–‡ç« : {article['file_name']} ({article['word_count']} < {min_length})")

        if len(valid_articles) < 2:
            print("ğŸ“Š æœ‰æ•ˆæ–‡ç« æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼åº¦æ¯”è¾ƒ")
            return {'duplicate_groups': [], 'unique_articles': articles, 'similarity_matrix': [], 'total_comparisons': 0}

        print(f"âœ… æœ‰æ•ˆæ–‡ç« : {len(valid_articles)} ç¯‡")

        # ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        print("\nğŸ”§ ç¬¬ä¸€é˜¶æ®µ: æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ...")
        similarity_matrix, total_comparisons = self._build_similarity_matrix(valid_articles)

        if self.debug_mode:
            print("\nğŸ“ˆ ç›¸ä¼¼åº¦çŸ©é˜µ (ä»…æ˜¾ç¤º >= 0.3 çš„ç›¸ä¼¼åº¦):")
            self._print_similarity_matrix(valid_articles, similarity_matrix, threshold=0.3)

        # ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºç›¸ä¼¼åº¦å›¾
        print("\nğŸ”§ ç¬¬äºŒé˜¶æ®µ: æ„å»ºç›¸ä¼¼åº¦å›¾...")
        similarity_graph = self._build_similarity_graph(valid_articles, similarity_matrix)

        # ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¾è¿é€šåˆ†é‡
        print("\nğŸ”§ ç¬¬ä¸‰é˜¶æ®µ: æŸ¥æ‰¾é‡å¤æ–‡ç« ç¾¤ç»„...")
        duplicate_groups = self._find_duplicate_groups(valid_articles, similarity_graph, similarity_matrix)

        # ç¬¬å››é˜¶æ®µï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š
        print("\nğŸ”§ ç¬¬å››é˜¶æ®µ: ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        unique_articles = self._identify_unique_articles(valid_articles, duplicate_groups)

        result = {
            'duplicate_groups': duplicate_groups,
            'unique_articles': unique_articles,
            'similarity_matrix': similarity_matrix,
            'total_comparisons': total_comparisons,
            'algorithm': 'graph_clustering'
        }

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nâœ… å…¨è¿æ¥å›¾æ£€æµ‹å®Œæˆ:")
        print(f"  ğŸ“Š æ€»æ¯”è¾ƒæ¬¡æ•°: {total_comparisons}")
        print(f"  ğŸ“¦ å‘ç°é‡å¤ç¾¤ç»„: {len(duplicate_groups)} ä¸ª")
        total_duplicates = sum(len(group['articles']) for group in duplicate_groups)
        print(f"  ğŸ”¸ é‡å¤æ–‡ç« æ€»æ•°: {total_duplicates} ç¯‡")
        print(f"  âœ… ç‹¬ç«‹æ–‡ç« : {len(unique_articles)} ç¯‡")

        return result

    def _build_similarity_matrix(self, articles: List[Dict]) -> Tuple[List[List[float]], int]:
        """æ„å»ºå®Œæ•´çš„ç›¸ä¼¼åº¦çŸ©é˜µ

        Args:
            articles: æœ‰æ•ˆæ–‡ç« åˆ—è¡¨

        Returns:
            (ç›¸ä¼¼åº¦çŸ©é˜µ, æ¯”è¾ƒæ¬¡æ•°)
        """
        n = len(articles)
        matrix = [[0.0 for _ in range(n)] for _ in range(n)]
        total_comparisons = 0

        print(f"  ğŸ”„ è®¡ç®— {n}Ã—{n} ç›¸ä¼¼åº¦çŸ©é˜µ...")

        for i in range(n):
            # å¯¹è§’çº¿ä¸º1.0 (è‡ªå·±ä¸è‡ªå·±å®Œå…¨ç›¸ä¼¼)
            matrix[i][i] = 1.0

            for j in range(i + 1, n):
                # æ£€æŸ¥æ—¶é—´çª—å£é™åˆ¶
                date_i = self._get_article_effective_date(articles[i])
                date_j = self._get_article_effective_date(articles[j])
                time_diff = abs((date_i - date_j).days)

                if time_diff > self.comparison_window_days:
                    matrix[i][j] = matrix[j][i] = 0.0
                    if self.debug_mode:
                        print(f"    ğŸ• è·³è¿‡æ—¶é—´çª—å£å¤–: {articles[i]['file_name']} vs {articles[j]['file_name']} (æ—¶é—´å·®{time_diff}å¤©)")
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._calculate_article_similarity(articles[i], articles[j])
                matrix[i][j] = matrix[j][i] = similarity
                total_comparisons += 1

                if self.debug_mode and similarity >= 0.5:
                    print(f"    ğŸ” é«˜ç›¸ä¼¼åº¦: {articles[i]['file_name']} vs {articles[j]['file_name']} = {similarity:.3f}")

        print(f"  âœ… çŸ©é˜µæ„å»ºå®Œæˆï¼Œå…±è®¡ç®— {total_comparisons} å¯¹æ–‡ç« ")
        return matrix, total_comparisons

    def _build_similarity_graph(self, articles: List[Dict], similarity_matrix: List[List[float]]) -> Dict[int, List[int]]:
        """åŸºäºç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå›¾

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ

        Returns:
            é‚»æ¥è¡¨è¡¨ç¤ºçš„å›¾
        """
        n = len(articles)
        graph = {i: [] for i in range(n)}
        edge_count = 0

        for i in range(n):
            for j in range(i + 1, n):
                # æ£€æŸ¥æ˜¯å¦è·¨ä¸»é¢˜
                is_cross_topic = self._are_cross_topic_articles(articles[i], articles[j])
                effective_threshold = self.cross_topic_threshold if is_cross_topic else self.similarity_threshold

                if similarity_matrix[i][j] >= effective_threshold:
                    graph[i].append(j)
                    graph[j].append(i)
                    edge_count += 1

                    if self.debug_mode:
                        print(f"    ğŸ”— è¿æ¥: {articles[i]['file_name']} â†” {articles[j]['file_name']} (ç›¸ä¼¼åº¦: {similarity_matrix[i][j]:.3f}, é˜ˆå€¼: {effective_threshold:.3f})")

        print(f"  âœ… ç›¸ä¼¼åº¦å›¾æ„å»ºå®Œæˆï¼ŒåŒ…å« {edge_count} æ¡è¾¹")
        return graph

    def _find_duplicate_groups(self, articles: List[Dict], graph: Dict[int, List[int]], similarity_matrix: List[List[float]]) -> List[Dict]:
        """ä½¿ç”¨DFSæŸ¥æ‰¾è¿é€šåˆ†é‡ï¼Œè¯†åˆ«é‡å¤æ–‡ç« ç¾¤ç»„

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            graph: ç›¸ä¼¼åº¦å›¾
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ

        Returns:
            é‡å¤ç¾¤ç»„åˆ—è¡¨
        """
        n = len(articles)
        visited = [False] * n
        duplicate_groups = []

        def dfs(node: int, component: List[int]):
            """æ·±åº¦ä¼˜å…ˆæœç´¢æ”¶é›†è¿é€šåˆ†é‡"""
            visited[node] = True
            component.append(node)

            for neighbor in graph[node]:
                if not visited[neighbor]:
                    dfs(neighbor, component)

        # æ‰¾æ‰€æœ‰è¿é€šåˆ†é‡
        for i in range(n):
            if not visited[i]:
                component = []
                dfs(i, component)

                # åªå…³å¿ƒåŒ…å«2ç¯‡ä»¥ä¸Šæ–‡ç« çš„ç¾¤ç»„(é‡å¤ç¾¤ç»„)
                if len(component) > 1:
                    # æŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ—©çš„ä½œä¸ºåŸºå‡†æ–‡ç« 
                    component_articles = [articles[idx] for idx in component]
                    component_sorted = sorted(component, key=lambda idx: self._get_article_effective_date(articles[idx]))
                    base_article_idx = component_sorted[0]

                    # æ„å»ºç¾¤ç»„ä¿¡æ¯
                    group_info = {
                        'base_article': articles[base_article_idx],
                        'articles': [],
                        'group_id': len(duplicate_groups) + 1,
                        'topic': self._classify_article_topic(articles[base_article_idx]) or 'Unknown'
                    }

                    for idx in component_sorted:
                        article_info = {
                            **articles[idx],
                            'similarity_to_base': similarity_matrix[base_article_idx][idx] if idx != base_article_idx else 1.0,
                            'is_base': idx == base_article_idx
                        }
                        group_info['articles'].append(article_info)

                    duplicate_groups.append(group_info)

                    if self.debug_mode:
                        print(f"    ğŸ“¦ å‘ç°ç¾¤ç»„ {len(duplicate_groups)}: {len(component)} ç¯‡æ–‡ç« ")
                        for idx in component_sorted:
                            marker = "ğŸ”¹" if idx == base_article_idx else "ğŸ”¸"
                            sim = similarity_matrix[base_article_idx][idx] if idx != base_article_idx else 1.0
                            print(f"      {marker} {articles[idx]['file_name']} (ç›¸ä¼¼åº¦: {sim:.3f})")

        print(f"  âœ… è¿é€šåˆ†é‡åˆ†æå®Œæˆï¼Œå‘ç° {len(duplicate_groups)} ä¸ªé‡å¤ç¾¤ç»„")
        return duplicate_groups

    def _identify_unique_articles(self, articles: List[Dict], duplicate_groups: List[Dict]) -> List[Dict]:
        """è¯†åˆ«ä¸å±äºä»»ä½•é‡å¤ç¾¤ç»„çš„ç‹¬ç«‹æ–‡ç« 

        Args:
            articles: æ‰€æœ‰æ–‡ç« åˆ—è¡¨
            duplicate_groups: é‡å¤ç¾¤ç»„åˆ—è¡¨

        Returns:
            ç‹¬ç«‹æ–‡ç« åˆ—è¡¨
        """
        # æ”¶é›†æ‰€æœ‰å±äºé‡å¤ç¾¤ç»„çš„æ–‡ç« 
        grouped_filenames = set()
        for group in duplicate_groups:
            for article in group['articles']:
                grouped_filenames.add(article['file_name'])

        # æ‰¾å‡ºä¸å±äºä»»ä½•ç¾¤ç»„çš„æ–‡ç« 
        unique_articles = [article for article in articles if article['file_name'] not in grouped_filenames]

        print(f"  âœ… è¯†åˆ«å‡º {len(unique_articles)} ç¯‡ç‹¬ç«‹æ–‡ç« ")
        return unique_articles

    def _print_similarity_matrix(self, articles: List[Dict], matrix: List[List[float]], threshold: float = 0.5):
        """æ‰“å°ç›¸ä¼¼åº¦çŸ©é˜µçš„å¯è§†åŒ–ç‰ˆæœ¬

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
            threshold: æ˜¾ç¤ºé˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ä¸æ˜¾ç¤º
        """
        n = len(articles)
        if n > 10:
            print(f"    çŸ©é˜µè¿‡å¤§({n}Ã—{n})ï¼Œä»…æ˜¾ç¤ºå‰10Ã—10éƒ¨åˆ†")
            display_n = 10
        else:
            display_n = n

        # æ‰“å°è¡¨å¤´
        print("    " + "".join(f"{i:>8}" for i in range(display_n)))

        # æ‰“å°çŸ©é˜µå†…å®¹
        for i in range(display_n):
            row_values = []
            for j in range(display_n):
                value = matrix[i][j]
                if value >= threshold:
                    row_values.append(f"{value:.3f}")
                else:
                    row_values.append("  -  ")
            print(f"{i:>2}: " + "".join(f"{val:>8}" for val in row_values))

    def compare_two_articles(self, file1: str, file2: str) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸¤ç¯‡æŒ‡å®šæ–‡ç« çš„ç›¸ä¼¼åº¦

        Args:
            file1: ç¬¬ä¸€ç¯‡æ–‡ç« æ–‡ä»¶è·¯å¾„
            file2: ç¬¬äºŒç¯‡æ–‡ç« æ–‡ä»¶è·¯å¾„

        Returns:
            è¯¦ç»†çš„å¯¹æ¯”ç»“æœ
        """
        print(f"ğŸ” å¯¹æ¯”ä¸¤ç¯‡æ–‡ç« :")
        print(f"  æ–‡ç« A: {file1}")
        print(f"  æ–‡ç« B: {file2}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(file1).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file1}")
            return None

        if not Path(file2).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file2}")
            return None

        # æå–æ–‡ç« ä¿¡æ¯
        article1 = self._extract_article_info(Path(file1))
        article2 = self._extract_article_info(Path(file2))

        if not article1 or not article2:
            print(f"âŒ æ— æ³•è§£ææ–‡ç« å†…å®¹")
            return None

        # è®¡ç®—å„ç§ç›¸ä¼¼åº¦
        title_similarity = self._calculate_title_similarity(article1['title'], article2['title'])
        content_similarity = self._calculate_content_similarity(article1['content'], article2['content'])
        overall_similarity = self._calculate_article_similarity(article1, article2)

        # åˆ¤æ–­æ˜¯å¦è·¨ä¸»é¢˜
        is_cross_topic = self._are_cross_topic_articles(article1, article2)
        effective_threshold = self.cross_topic_threshold if is_cross_topic else self.similarity_threshold

        # æ„å»ºç»“æœ
        result = {
            'article1': {
                'file_path': file1,
                'title': article1['title'],
                'word_count': article1['word_count'],
                'effective_date': self._get_article_effective_date(article1)
            },
            'article2': {
                'file_path': file2,
                'title': article2['title'],
                'word_count': article2['word_count'],
                'effective_date': self._get_article_effective_date(article2)
            },
            'similarity_scores': {
                'title_similarity': title_similarity,
                'content_similarity': content_similarity,
                'overall_similarity': overall_similarity
            },
            'analysis': {
                'is_cross_topic': is_cross_topic,
                'effective_threshold': effective_threshold,
                'is_similar': overall_similarity >= effective_threshold,
                'topic1': self._classify_article_topic(article1),
                'topic2': self._classify_article_topic(article2)
            },
            'details': {
                'word_count_diff': abs(article1['word_count'] - article2['word_count']),
                'char_count_diff': abs(article1['char_count'] - article2['char_count']),
            }
        }

        # è¾“å‡ºç»“æœ
        print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†æç»“æœ:")
        print(f"  æ ‡é¢˜ç›¸ä¼¼åº¦: {title_similarity:.3f}")
        print(f"  å†…å®¹ç›¸ä¼¼åº¦: {content_similarity:.3f}")
        print(f"  ç»¼åˆç›¸ä¼¼åº¦: {overall_similarity:.3f}")
        print(f"  æœ‰æ•ˆé˜ˆå€¼: {effective_threshold:.3f} ({'è·¨ä¸»é¢˜' if is_cross_topic else 'åŒä¸»é¢˜'})")
        print(f"  åˆ¤æ–­ç»“æœ: {'ç›¸ä¼¼' if result['analysis']['is_similar'] else 'ä¸ç›¸ä¼¼'}")

        if result['analysis']['is_similar']:
            print(f"  âœ… ç›¸ä¼¼åº¦ {overall_similarity:.3f} â‰¥ é˜ˆå€¼ {effective_threshold:.3f}")
        else:
            print(f"  âŒ ç›¸ä¼¼åº¦ {overall_similarity:.3f} < é˜ˆå€¼ {effective_threshold:.3f}")

        return result

    def _calculate_article_similarity(self, article1: Dict, article2: Dict) -> float:
        """è®¡ç®—ä¸¤ç¯‡æ–‡ç« çš„æ•´ä½“ç›¸ä¼¼åº¦"""
        # æ£€æŸ¥æ˜¯å¦å®Œå…¨ç›¸åŒ
        if article1['content_hash'] == article2['content_hash']:
            return 1.0

        # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
        title_sim = 0.0
        if self.config.get('check_title_similarity', True):
            title_sim = self._calculate_title_similarity(article1['title'], article2['title'])

        # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
        content_sim = 0.0
        if self.config.get('check_content_similarity', True):
            content_sim = self._calculate_content_similarity(article1['content'], article2['content'])

        # åŠ æƒè®¡ç®—æ€»ç›¸ä¼¼åº¦
        title_weight = self.config.get('title_weight', 0.3)
        content_weight = self.config.get('content_weight', 0.7)

        total_similarity = title_sim * title_weight + content_sim * content_weight
        return min(1.0, total_similarity)

    def _calculate_title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
        if not title1 or not title2:
            return 0.0

        # æ ‡å‡†åŒ–æ ‡é¢˜
        title1_clean = self._normalize_text(title1)
        title2_clean = self._normalize_text(title2)

        if title1_clean == title2_clean:
            return 1.0

        # è®¡ç®—è¯æ±‡é‡å åº¦
        words1 = set(title1_clean.split())
        words2 = set(title2_clean.split())

        if not words1 or not words2:
            return 0.0

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0

    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨TF-IDFå’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        if not content1 or not content2:
            return 0.0

        # æ ‡å‡†åŒ–å†…å®¹
        content1_clean = self._normalize_text(content1)
        content2_clean = self._normalize_text(content2)

        if content1_clean == content2_clean:
            return 1.0

        # ç®€å•çš„è¯é¢‘ç»Ÿè®¡æ–¹æ³•
        words1 = content1_clean.split()
        words2 = content2_clean.split()

        # æ„å»ºè¯æ±‡è¡¨
        vocab = set(words1 + words2)

        if not vocab:
            return 0.0

        # è®¡ç®—è¯é¢‘å‘é‡
        vec1 = [words1.count(word) for word in vocab]
        vec2 = [words2.count(word) for word in vocab]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a * a for a in vec1) ** 0.5
        magnitude2 = sum(b * b for b in vec2) ** 0.5

        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        return dot_product / (magnitude1 * magnitude2)

    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        # è½¬ä¸ºå°å†™
        text = text.lower()

        # ç§»é™¤Markdownæ ‡è®°
        text = re.sub(r'[#*`\[\]()]', '', text)
        text = re.sub(r'http[s]?://\S+', '', text)

        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s-]', ' ', text)

        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)

        return text.strip()

    def _extract_date_from_filename(self, filename: str) -> Optional[datetime]:
        """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ

        æ”¯æŒæ ¼å¼ï¼šxxx-20250914.md, xxx-2025-09-14.md ç­‰
        """
        import re

        # åŒ¹é…YYYYMMDDæ ¼å¼
        date_pattern1 = re.search(r'(\d{8})', filename)
        if date_pattern1:
            try:
                date_str = date_pattern1.group(1)
                return datetime.strptime(date_str, '%Y%m%d')
            except ValueError:
                pass

        # åŒ¹é…YYYY-MM-DDæ ¼å¼
        date_pattern2 = re.search(r'(\d{4}-\d{2}-\d{2})', filename)
        if date_pattern2:
            try:
                date_str = date_pattern2.group(1)
                return datetime.strptime(date_str, '%Y-%m-%d')
            except ValueError:
                pass

        return None

    def _get_article_effective_date(self, article: Dict) -> datetime:
        """è·å–æ–‡ç« çš„æœ‰æ•ˆæ—¥æœŸï¼ˆç”¨äºæ’åºï¼‰

        ä¼˜å…ˆçº§ï¼šæ–‡ä»¶åæ—¥æœŸ > åˆ›å»ºæ—¶é—´ > ä¿®æ”¹æ—¶é—´
        """
        # 1. å°è¯•ä»æ–‡ä»¶åæå–æ—¥æœŸ
        filename_date = self._extract_date_from_filename(article['file_name'])
        if filename_date:
            return filename_date

        # 2. ä½¿ç”¨åˆ›å»ºæ—¶é—´
        if 'created_time' in article:
            return article['created_time']

        # 3. ä½¿ç”¨ä¿®æ”¹æ—¶é—´ä½œä¸ºåå¤‡
        return article['modified_time']

    def _classify_article_topic(self, article: Dict) -> Optional[str]:
        """å¯¹æ–‡ç« è¿›è¡Œä¸»é¢˜åˆ†ç±»

        Args:
            article: æ–‡ç« ä¿¡æ¯

        Returns:
            ä¸»é¢˜ç±»åˆ«åç§°ï¼Œå¦‚æœæ— æ³•åˆ†ç±»åˆ™è¿”å›None
        """
        if not self.topic_classification.get('enabled', False):
            return None

        topics = self.topic_classification.get('topics', {})
        if not topics:
            return None

        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹è¿›è¡Œåˆ†ç±»
        text_to_classify = (article['title'] + ' ' + article['content']).lower()

        # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„åŒ¹é…åˆ†æ•°
        topic_scores = {}
        for topic_name, topic_config in topics.items():
            keywords = topic_config.get('keywords', [])
            if not keywords:
                continue

            score = 0
            for keyword in keywords:
                if keyword.lower() in text_to_classify:
                    score += 1

            if score > 0:
                # æ ‡å‡†åŒ–åˆ†æ•° (åŒ¹é…å…³é”®è¯æ•° / æ€»å…³é”®è¯æ•°)
                topic_scores[topic_name] = score / len(keywords)

        if not topic_scores:
            return None

        # è¿”å›å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
        best_topic = max(topic_scores.items(), key=lambda x: x[1])
        return best_topic[0] if best_topic[1] > 0.1 else None  # è‡³å°‘10%å…³é”®è¯åŒ¹é…

    def _are_cross_topic_articles(self, article1: Dict, article2: Dict) -> bool:
        """åˆ¤æ–­ä¸¤ç¯‡æ–‡ç« æ˜¯å¦å±äºä¸åŒä¸»é¢˜

        Args:
            article1, article2: æ–‡ç« ä¿¡æ¯

        Returns:
            Trueå¦‚æœå±äºä¸åŒä¸»é¢˜
        """
        if not self.topic_classification.get('enabled', False):
            return False

        topic1 = self._classify_article_topic(article1)
        topic2 = self._classify_article_topic(article2)

        # å¦‚æœä»»ä¸€æ–‡ç« æ— æ³•åˆ†ç±»ï¼Œä¸è®¤ä¸ºæ˜¯è·¨ä¸»é¢˜
        if topic1 is None or topic2 is None:
            return False

        return topic1 != topic2

    def process_articles_by_date(self, processing_result: Dict[str, Any],
                                move_files: bool = True) -> Dict[str, Any]:
        """æŒ‰æ—¥æœŸå¤„ç†æ–‡ç« æ–‡ä»¶

        Args:
            processing_result: æ£€æµ‹ç»“æœ
            move_files: æ˜¯å¦å®é™…ç§»åŠ¨æ–‡ä»¶

        Returns:
            å¤„ç†ç»“æœæŠ¥å‘Š
        """
        kept_articles = processing_result['kept_articles']
        moved_articles = processing_result['moved_articles']
        processing_date = processing_result['processing_date']

        if not moved_articles:
            print("ğŸ“Š æ²¡æœ‰éœ€è¦ç§»åŠ¨çš„æ–‡ç« ")
            return {
                'moved_count': 0,
                'kept_count': len(kept_articles),
                'new_articles_folder': f"{self.new_articles_folder}/{processing_date}",
                'old_articles_folder': f"{self.old_articles_folder}/{processing_date}"
            }

        # åˆ›å»ºæŒ‰æ—¥æœŸåˆ†ç±»çš„æ–‡ä»¶å¤¹
        new_articles_dir = Path(f"{self.new_articles_folder}/{processing_date}")
        old_articles_dir = Path(f"{self.old_articles_folder}/{processing_date}")

        if move_files:
            new_articles_dir.mkdir(parents=True, exist_ok=True)
            old_articles_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ“ åˆ›å»ºæ–‡ä»¶å¤¹:")
            print(f"  âœ… ä¿ç•™æ–‡ç« : {new_articles_dir.absolute()}")
            print(f"  ğŸ“¦ ç›¸ä¼¼æ–‡ç« : {old_articles_dir.absolute()}")

        moved_count = 0
        kept_count = 0

        # 1. ç§»åŠ¨ä¿ç•™çš„æ–‡ç« åˆ°new-articles
        print(f"\nğŸ“ å¤„ç†ä¿ç•™æ–‡ç«  ({len(kept_articles)} ç¯‡):")
        for article in kept_articles:
            if move_files:
                try:
                    source_path = Path(article['file_path'])
                    target_path = new_articles_dir / source_path.name

                    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
                    if target_path.exists():
                        timestamp = datetime.now().strftime('%H%M%S')
                        stem = target_path.stem
                        suffix = target_path.suffix
                        target_name = f"{stem}_{timestamp}{suffix}"
                        target_path = new_articles_dir / target_name

                    shutil.move(str(source_path), str(target_path))
                    print(f"  âœ… ç§»åŠ¨: {article['file_name']} â†’ new-articles/{processing_date}/")
                    kept_count += 1

                except Exception as e:
                    print(f"  âŒ ç§»åŠ¨å¤±è´¥ {article['file_name']}: {e}")
            else:
                print(f"  âœ… å¾…ç§»åŠ¨: {article['file_name']} â†’ new-articles/{processing_date}/")
                kept_count += 1

        # 2. ç§»åŠ¨ç›¸ä¼¼çš„æ–‡ç« åˆ°old-articles
        print(f"\nğŸ“¦ å¤„ç†ç›¸ä¼¼æ–‡ç«  ({len(moved_articles)} ç¯‡):")
        for article in moved_articles:
            if move_files:
                try:
                    source_path = Path(article['file_path'])
                    target_path = old_articles_dir / source_path.name

                    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
                    if target_path.exists():
                        timestamp = datetime.now().strftime('%H%M%S')
                        stem = target_path.stem
                        suffix = target_path.suffix
                        target_name = f"{stem}_{timestamp}{suffix}"
                        target_path = old_articles_dir / target_name

                    shutil.move(str(source_path), str(target_path))
                    similarity = article.get('similarity_to_base', 0)
                    base_article = article.get('base_article', 'unknown')
                    print(f"  ğŸ“¦ ç§»åŠ¨: {article['file_name']} â†’ old-articles/{processing_date}/ (ç›¸ä¼¼åº¦: {similarity:.3f}, åŸºå‡†: {base_article})")
                    moved_count += 1

                except Exception as e:
                    print(f"  âŒ ç§»åŠ¨å¤±è´¥ {article['file_name']}: {e}")
            else:
                similarity = article.get('similarity_to_base', 0)
                base_article = article.get('base_article', 'unknown')
                print(f"  ğŸ“¦ å¾…ç§»åŠ¨: {article['file_name']} â†’ old-articles/{processing_date}/ (ç›¸ä¼¼åº¦: {similarity:.3f}, åŸºå‡†: {base_article})")
                moved_count += 1

        result = {
            'moved_count': moved_count,
            'kept_count': kept_count,
            'new_articles_folder': str(new_articles_dir.absolute()) if move_files else f"{self.new_articles_folder}/{processing_date}",
            'old_articles_folder': str(old_articles_dir.absolute()) if move_files else f"{self.old_articles_folder}/{processing_date}",
            'processing_date': processing_date
        }

        print(f"\nâœ… æ–‡ç« åˆ†ç±»å¤„ç†å®Œæˆ:")
        print(f"  âœ… ä¿ç•™æ–‡ç« : {kept_count} ç¯‡")
        print(f"  ğŸ“¦ ç§»åŠ¨æ–‡ç« : {moved_count} ç¯‡")

        return result


    def generate_duplicate_analysis_report(self, detection_result: Dict[str, Any], output_file: str = "duplicate_analysis_report.md") -> str:
        """ä¸ºå…¨è¿æ¥å›¾ç®—æ³•ç»“æœç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š

        Args:
            detection_result: å…¨è¿æ¥å›¾æ£€æµ‹ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ“„ æ­£åœ¨ç”Ÿæˆé‡å¤æ–‡ç« åˆ†ææŠ¥å‘Š...")

        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        duplicate_groups = detection_result.get('duplicate_groups', [])
        unique_articles = detection_result.get('unique_articles', [])
        total_comparisons = detection_result.get('total_comparisons', 0)
        algorithm = detection_result.get('algorithm', 'graph_clustering')

        # ç»Ÿè®¡ä¿¡æ¯
        total_duplicates = sum(len(group['articles']) for group in duplicate_groups)
        total_articles = total_duplicates + len(unique_articles)

        report_parts = []

        # æŠ¥å‘Šå¤´éƒ¨
        report_parts.append(f"""# ğŸ“Š é‡å¤æ–‡ç« ç¾¤ç»„åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}
**æ£€æµ‹ç®—æ³•**: {algorithm} (å…¨è¿æ¥å›¾èšç±»)
**æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold}
**æ€»æ–‡ç« æ•°**: {total_articles}
**æ¯”è¾ƒæ¬¡æ•°**: {total_comparisons}
**é‡å¤ç¾¤ç»„**: {len(duplicate_groups)} ä¸ª
**é‡å¤æ–‡ç« **: {total_duplicates} ç¯‡
**ç‹¬ç«‹æ–‡ç« **: {len(unique_articles)} ç¯‡
**é‡å¤ç‡**: {(total_duplicates/total_articles*100):.1f}%

---""")

        # æ‰§è¡Œæ‘˜è¦
        if duplicate_groups:
            report_parts.append(f"""
## ğŸ¯ æ‰§è¡Œæ‘˜è¦

æœ¬æ¬¡æ£€æµ‹ä½¿ç”¨å…¨è¿æ¥å›¾èšç±»ç®—æ³•ï¼Œç¡®ä¿æ•°å­¦å®Œæ•´æ€§ï¼Œæ— é—æ¼ä»»ä½•ç›¸ä¼¼å…³ç³»ã€‚

### å…³é”®å‘ç°
- ğŸ“¦ å‘ç° **{len(duplicate_groups)}** ä¸ªé‡å¤æ–‡ç« ç¾¤ç»„
- ğŸ”¸ æ¶‰åŠ **{total_duplicates}** ç¯‡é‡å¤æ–‡ç« 
- âœ… **{len(unique_articles)}** ç¯‡æ–‡ç« å®Œå…¨ç‹¬ç«‹
- ğŸ“ˆ å¹³å‡æ¯ç¾¤ç»„ **{(total_duplicates/len(duplicate_groups)):.1f}** ç¯‡æ–‡ç« 

### ç®—æ³•ä¼˜åŠ¿
- âœ… **æ•°å­¦å®Œæ•´æ€§**: æ‰€æœ‰æ–‡ç« å¯¹éƒ½è¿›è¡Œäº†æ¯”è¾ƒåˆ†æ
- âœ… **ä¼ é€’æ€§å¤„ç†**: Aâ†’Bâ†’Cå…³ç³»è¢«æ­£ç¡®è¯†åˆ«ä¸ºä¸€ä¸ªç¾¤ç»„
- âœ… **é›¶é—æ¼**: è¿é€šåˆ†é‡ç®—æ³•ç¡®ä¿ä¸é—æ¼ä»»ä½•ç›¸ä¼¼å…³ç³»
- âœ… **SEOå‹å¥½**: æ¯ä¸ªç¾¤ç»„ä¿ç•™æœ€æ—©å‘å¸ƒçš„æ–‡ç« ï¼ˆSEOä»·å€¼æœ€é«˜ï¼‰
""")

        # é‡å¤ç¾¤ç»„è¯¦æƒ…
        if duplicate_groups:
            report_parts.append("\n## ğŸ“¦ é‡å¤æ–‡ç« ç¾¤ç»„è¯¦æƒ…\n")

            for i, group in enumerate(duplicate_groups, 1):
                base_article = group['base_article']
                articles = group['articles']
                topic = group.get('topic', 'Unknown')

                # ç¾¤ç»„æ ‡é¢˜
                base_date = self._get_article_effective_date(base_article).strftime('%Y-%m-%d')
                report_parts.append(f"""### ç¾¤ç»„ {i}: {topic} ä¸»é¢˜ ({len(articles)}ç¯‡æ–‡ç« )

**åŸºå‡†æ–‡ç« **: `{base_article['file_name']}` (å‘å¸ƒæ—¥æœŸ: {base_date})
**ç¾¤ç»„ä¸»é¢˜**: {topic}
**ç›¸ä¼¼åº¦èŒƒå›´**: {min(a['similarity_to_base'] for a in articles if not a['is_base']):.3f} - {max(a['similarity_to_base'] for a in articles if not a['is_base']):.3f}

#### æ–‡ç« åˆ—è¡¨:""")

                # æ–‡ç« è¯¦æƒ…
                for article in articles:
                    if article['is_base']:
                        marker = "ğŸ”¹ **åŸºå‡†**"
                        sim_text = "1.000"
                    else:
                        marker = "ğŸ”¸"
                        sim_text = f"{article['similarity_to_base']:.3f}"

                    article_date = self._get_article_effective_date(article).strftime('%Y-%m-%d')
                    report_parts.append(f"- {marker} `{article['file_name']}` (ç›¸ä¼¼åº¦: {sim_text}, æ—¥æœŸ: {article_date})")

                report_parts.append("")  # ç©ºè¡Œåˆ†éš”

        # SEOä¼˜åŒ–å»ºè®®éƒ¨åˆ†
        if duplicate_groups:
            report_parts.append(f"""
## ğŸ”§ SEOä¼˜åŒ–å¤„ç†å»ºè®®

åŸºäºarticle_similarity_guide.mdçš„æœ€ä½³å®è·µï¼Œä¸ºå‘ç°çš„{len(duplicate_groups)}ä¸ªé‡å¤ç¾¤ç»„æä¾›å…·ä½“çš„SEOä¼˜åŒ–æ–¹æ¡ˆï¼š

### ğŸ“‹ å¤„ç†ç­–ç•¥æ€»è§ˆ

1. **301é‡å®šå‘ (æ¨è)**: åˆå¹¶å†…å®¹ç›¸ä¼¼åº¦>0.8çš„æ–‡ç« ï¼Œè®¾ç½®æ°¸ä¹…é‡å®šå‘ä¿æŠ¤SEOæƒé‡
2. **Canonicalæ ‡ç­¾**: ä¿ç•™ç›¸ä¼¼åº¦0.5-0.8ä¹‹é—´çš„æ–‡ç« ï¼Œä½¿ç”¨canonicalæ ‡ç­¾æŒ‡å‘ä¸»è¦ç‰ˆæœ¬
3. **å†…å®¹å·®å¼‚åŒ–**: ç›¸ä¼¼åº¦<0.7çš„æ–‡ç« å¯é€šè¿‡å†…å®¹è¡¥å……å®ç°å·®å¼‚åŒ–

---""")

            # ä¸ºæ¯ä¸ªç¾¤ç»„ç”Ÿæˆå…·ä½“çš„SEOå»ºè®®
            for i, group in enumerate(duplicate_groups, 1):
                base_article = group['base_article']
                articles = group['articles']
                topic = group.get('topic', 'Unknown')

                # è®¡ç®—ç¾¤ç»„å†…æœ€é«˜ç›¸ä¼¼åº¦
                non_base_articles = [a for a in articles if not a['is_base']]
                if non_base_articles:
                    max_similarity = max(a['similarity_to_base'] for a in non_base_articles)
                    avg_similarity = sum(a['similarity_to_base'] for a in non_base_articles) / len(non_base_articles)
                else:
                    max_similarity = avg_similarity = 0.0

                report_parts.append(f"""
### ç¾¤ç»„ {i} å¤„ç†å»ºè®®: {topic}

**åŸºå‡†æ–‡ç« **: `{base_article['file_name']}`
**ç¾¤ç»„ç›¸ä¼¼åº¦**: å¹³å‡ {avg_similarity:.3f}, æœ€é«˜ {max_similarity:.3f}

#### ğŸ¯ æ¨èç­–ç•¥:""")

                if max_similarity >= 0.8:
                    # é«˜ç›¸ä¼¼åº¦ï¼šå»ºè®®301é‡å®šå‘
                    report_parts.append(f"""
**301é‡å®šå‘æ–¹æ¡ˆ** (ç›¸ä¼¼åº¦ â‰¥ 0.8)
- âœ… **ä¿ç•™**: `{base_article['file_name']}` (åŸºå‡†æ–‡ç« )
- ğŸ”„ **é‡å®šå‘åˆ°åŸºå‡†æ–‡ç« **:""")
                    for article in non_base_articles:
                        if article['similarity_to_base'] >= 0.8:
                            report_parts.append(f"  - `{article['file_name']}` â†’ `{base_article['file_name']}`")

                    report_parts.append(f"""
**æ“ä½œæ­¥éª¤**:
1. å°†é‡å¤æ–‡ç« çš„ä¼˜è´¨å†…å®¹åˆå¹¶åˆ°åŸºå‡†æ–‡ç«  `{base_article['file_name']}`
2. åœ¨ç½‘ç«™é…ç½®ä¸­æ·»åŠ 301é‡å®šå‘è§„åˆ™
3. åˆ é™¤é‡å¤æ–‡ç« æ–‡ä»¶
4. æ›´æ–°å†…éƒ¨é“¾æ¥æŒ‡å‘åŸºå‡†æ–‡ç« """)

                elif max_similarity >= 0.5:
                    # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šå»ºè®®Canonicalæ ‡ç­¾
                    report_parts.append(f"""
**Canonicalæ ‡ç­¾æ–¹æ¡ˆ** (ç›¸ä¼¼åº¦ 0.5-0.8)
- âœ… **ä¸»è¦æ–‡ç« **: `{base_article['file_name']}`
- ğŸ·ï¸ **è®¾ç½®Canonicalæ ‡ç­¾**:""")
                    for article in non_base_articles:
                        if 0.5 <= article['similarity_to_base'] < 0.8:
                            report_parts.append(f"  - `{article['file_name']}` â†’ canonicalæŒ‡å‘ `{base_article['file_name']}`")

                    report_parts.append(f"""
**HTMLæ ‡ç­¾ç¤ºä¾‹**:
```html
<link rel="canonical" href="/articles/{base_article['file_name'].replace('.md', '.html')}" />
```

**æ“ä½œè¯´æ˜**:
1. ä¿ç•™æ‰€æœ‰æ–‡ç« ï¼Œä¸åˆ é™¤ä»»ä½•å†…å®¹
2. åœ¨æ¬¡è¦æ–‡ç« çš„HTMLå¤´éƒ¨æ·»åŠ canonicalæ ‡ç­¾
3. å‘Šè¯‰æœç´¢å¼•æ“ä»¥åŸºå‡†æ–‡ç« ä¸ºå‡†è¿›è¡Œç´¢å¼•""")

                else:
                    # ä½ç›¸ä¼¼åº¦ï¼šå»ºè®®å†…å®¹å·®å¼‚åŒ–
                    report_parts.append(f"""
**å†…å®¹å·®å¼‚åŒ–æ–¹æ¡ˆ** (ç›¸ä¼¼åº¦ < 0.5)
- ğŸ“ **å·®å¼‚åŒ–å¤„ç†**: å„æ–‡ç« ä¿æŒç‹¬ç«‹ï¼Œå¢å¼ºå†…å®¹å·®å¼‚æ€§

**ä¼˜åŒ–å»ºè®®**:
1. ä¸ºæ¯ç¯‡æ–‡ç« è¡¥å……ä¸åŒçš„æ¡ˆä¾‹ã€æ•°æ®æˆ–è§‚ç‚¹
2. è°ƒæ•´æ–‡ç« è§’åº¦ï¼šæŠ€æœ¯å®ç° vs ç”¨æˆ·æŒ‡å— vs äº§å“æ¯”è¾ƒ
3. æ·»åŠ ç‹¬ç‰¹çš„åº”ç”¨åœºæ™¯æˆ–è§£å†³æ–¹æ¡ˆ
4. ç¡®ä¿æ¯ç¯‡æ–‡ç« éƒ½æœ‰æ˜ç¡®çš„ç›®æ ‡å—ä¼—""")

                report_parts.append("")

        # ç‹¬ç«‹æ–‡ç« åˆ—è¡¨
        if unique_articles:
            report_parts.append(f"""
## âœ… ç‹¬ç«‹æ–‡ç« åˆ—è¡¨ ({len(unique_articles)}ç¯‡)

ä»¥ä¸‹æ–‡ç« ä¸å…¶ä»–æ–‡ç« ä¸å­˜åœ¨æ˜¾è‘—ç›¸ä¼¼æ€§ï¼Œä¸ºç‹¬ç«‹åŸåˆ›å†…å®¹ï¼š

""")
            for article in unique_articles:
                article_date = self._get_article_effective_date(article).strftime('%Y-%m-%d')
                word_count = article.get('word_count', 0)
                report_parts.append(f"- `{article['file_name']}` (æ—¥æœŸ: {article_date}, å­—æ•°: {word_count})")

        # æŠ€æœ¯è¯´æ˜
        report_parts.append(f"""

---

## ğŸ”¬ æŠ€æœ¯è¯´æ˜

### ç®—æ³•åŸç†
1. **ç›¸ä¼¼åº¦çŸ©é˜µæ„å»º**: è®¡ç®—æ‰€æœ‰æ–‡ç« å¯¹çš„TF-IDFä½™å¼¦ç›¸ä¼¼åº¦
2. **å›¾æ„å»º**: ç›¸ä¼¼åº¦â‰¥{self.similarity_threshold}çš„æ–‡ç« å¯¹å½¢æˆè¿æ¥
3. **è¿é€šåˆ†é‡**: ä½¿ç”¨DFSç®—æ³•æ‰¾å‡ºæ‰€æœ‰è¿é€šåˆ†é‡(é‡å¤ç¾¤ç»„)
4. **åŸºå‡†é€‰æ‹©**: æ¯ä¸ªç¾¤ç»„é€‰æ‹©æœ€æ—©å‘å¸ƒçš„æ–‡ç« ä½œä¸ºåŸºå‡†ä¿ç•™

### ç›¸ä¼¼åº¦è®¡ç®—
- **æ ‡é¢˜æƒé‡**: 30%
- **å†…å®¹æƒé‡**: 70%
- **åŒä¸»é¢˜é˜ˆå€¼**: {self.similarity_threshold}
- **è·¨ä¸»é¢˜é˜ˆå€¼**: {getattr(self, 'cross_topic_threshold', 0.7)}

### æ—¶é—´çª—å£
- **æ£€æµ‹çª—å£**: {self.comparison_window_days} å¤©
- **å­—æ•°é—¨æ§›**: {self.config.get('min_content_length', 1000)} å­—

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}*
*ç®—æ³•ç‰ˆæœ¬: Graph Clustering v2.3*""")

        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        report_content = '\n'.join(report_parts)

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            print(f"âœ… é‡å¤æ–‡ç« åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            return output_file

        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def generate_seo_config_files(self, detection_result: Dict[str, Any], output_dir: str = ".") -> Dict[str, str]:
        """ç”ŸæˆSEOä¼˜åŒ–é…ç½®æ–‡ä»¶ - redirects.yamlå’Œcanonical_mappings.json

        Args:
            detection_result: æ£€æµ‹ç»“æœ
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ç”Ÿæˆçš„é…ç½®æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        import json
        import yaml

        print(f"ğŸ“ æ­£åœ¨ç”ŸæˆSEOé…ç½®æ–‡ä»¶...")

        duplicate_groups = detection_result.get('duplicate_groups', [])

        # 301é‡å®šå‘é…ç½®
        redirects_config = {'redirects': []}

        # Canonicalæ ‡ç­¾æ˜ å°„
        canonical_mappings = {}

        # å†…å®¹å·®å¼‚åŒ–å»ºè®®
        differentiation_suggestions = []

        for group in duplicate_groups:
            base_article = group['base_article']
            articles = group['articles']

            # è®¡ç®—ç¾¤ç»„å†…æœ€é«˜ç›¸ä¼¼åº¦
            non_base_articles = [a for a in articles if not a['is_base']]
            if not non_base_articles:
                continue

            for article in non_base_articles:
                similarity = article['similarity_to_base']
                source_path = f"/articles/{article['file_name'].replace('.md', '.html')}"
                target_path = f"/articles/{base_article['file_name'].replace('.md', '.html')}"

                if similarity >= 0.8:
                    # 301é‡å®šå‘
                    redirects_config['redirects'].append({
                        'from': source_path,
                        'to': target_path,
                        'status': 301,
                        'reason': f'é‡å¤å†…å®¹åˆå¹¶ (ç›¸ä¼¼åº¦: {similarity:.3f})',
                        'similarity': similarity
                    })

                elif similarity >= 0.5:
                    # Canonicalæ ‡ç­¾
                    canonical_mappings[article['file_name']] = {
                        'canonical_url': target_path,
                        'canonical_file': base_article['file_name'],
                        'similarity': similarity,
                        'html_tag': f'<link rel="canonical" href="{target_path}" />'
                    }

                else:
                    # å†…å®¹å·®å¼‚åŒ–å»ºè®®
                    differentiation_suggestions.append({
                        'file': article['file_name'],
                        'base_file': base_article['file_name'],
                        'similarity': similarity,
                        'suggestions': [
                            'ä¸ºæ–‡ç« è¡¥å……ä¸åŒçš„æ¡ˆä¾‹ã€æ•°æ®æˆ–è§‚ç‚¹',
                            'è°ƒæ•´æ–‡ç« è§’åº¦ï¼šæŠ€æœ¯å®ç° vs ç”¨æˆ·æŒ‡å— vs äº§å“æ¯”è¾ƒ',
                            'æ·»åŠ ç‹¬ç‰¹çš„åº”ç”¨åœºæ™¯æˆ–è§£å†³æ–¹æ¡ˆ',
                            'ç¡®ä¿æ–‡ç« æœ‰æ˜ç¡®çš„ç›®æ ‡å—ä¼—'
                        ]
                    })

        # ç”Ÿæˆé…ç½®æ–‡ä»¶
        config_files = {}

        # 1. ç”Ÿæˆredirects.yaml
        if redirects_config['redirects']:
            redirects_file = os.path.join(output_dir, 'redirects.yaml')
            try:
                with open(redirects_file, 'w', encoding='utf-8') as f:
                    yaml.dump(redirects_config, f, default_flow_style=False,
                            allow_unicode=True, sort_keys=False)
                config_files['redirects'] = redirects_file
                print(f"âœ… 301é‡å®šå‘é…ç½®å·²ç”Ÿæˆ: {redirects_file}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆredirects.yamlå¤±è´¥: {e}")

        # 2. ç”Ÿæˆcanonical_mappings.json
        if canonical_mappings:
            canonical_file = os.path.join(output_dir, 'canonical_mappings.json')
            try:
                with open(canonical_file, 'w', encoding='utf-8') as f:
                    json.dump(canonical_mappings, f, indent=2, ensure_ascii=False)
                config_files['canonical'] = canonical_file
                print(f"âœ… Canonicalæ ‡ç­¾é…ç½®å·²ç”Ÿæˆ: {canonical_file}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆcanonical_mappings.jsonå¤±è´¥: {e}")

        # 3. ç”Ÿæˆå·®å¼‚åŒ–å»ºè®®æ–‡ä»¶
        if differentiation_suggestions:
            diff_file = os.path.join(output_dir, 'content_differentiation.json')
            try:
                with open(diff_file, 'w', encoding='utf-8') as f:
                    json.dump(differentiation_suggestions, f, indent=2, ensure_ascii=False)
                config_files['differentiation'] = diff_file
                print(f"âœ… å†…å®¹å·®å¼‚åŒ–å»ºè®®å·²ç”Ÿæˆ: {diff_file}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆcontent_differentiation.jsonå¤±è´¥: {e}")

        # 4. ç”Ÿæˆå®æ–½è¯´æ˜æ–‡æ¡£
        readme_content = f"""# SEOä¼˜åŒ–é…ç½®æ–‡ä»¶ä½¿ç”¨è¯´æ˜

æœ¬ç›®å½•åŒ…å«è‡ªåŠ¨ç”Ÿæˆçš„SEOä¼˜åŒ–é…ç½®æ–‡ä»¶ï¼Œå¸®åŠ©æ‚¨å¤„ç†ç½‘ç«™ä¸­çš„é‡å¤å†…å®¹é—®é¢˜ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

### 1. redirects.yaml
**ç”¨é€”**: 301æ°¸ä¹…é‡å®šå‘é…ç½®
**é€‚ç”¨åœºæ™¯**: é«˜ç›¸ä¼¼åº¦æ–‡ç« (â‰¥0.8)çš„åˆå¹¶å¤„ç†

**Hugoå®æ–½æ–¹æ³•**:
```yaml
# åœ¨Hugoé…ç½®æ–‡ä»¶(config.yaml)ä¸­æ·»åŠ :
outputs:
  home: ["HTML", "RSS", "REDIRECTS"]

# å°†redirects.yamlå†…å®¹æ·»åŠ åˆ°static/_redirectsæ–‡ä»¶
```

**æ–‡ä»¶æ•°é‡**: {len(redirects_config['redirects'])} ä¸ªé‡å®šå‘è§„åˆ™

### 2. canonical_mappings.json
**ç”¨é€”**: Canonicalæ ‡ç­¾é…ç½®æ˜ å°„
**é€‚ç”¨åœºæ™¯**: ä¸­ç­‰ç›¸ä¼¼åº¦æ–‡ç« (0.5-0.8)çš„SEOä¼˜åŒ–

**å®æ–½æ–¹æ³•**:
1. åœ¨æ–‡ç« çš„Front Matterä¸­æ·»åŠ canonicalå­—æ®µ
2. æˆ–åœ¨æ¨¡æ¿æ–‡ä»¶çš„<head>éƒ¨åˆ†ä½¿ç”¨é…ç½®ç”Ÿæˆcanonicalæ ‡ç­¾

**æ–‡ä»¶æ•°é‡**: {len(canonical_mappings)} ä¸ªcanonicalæ˜ å°„

### 3. content_differentiation.json
**ç”¨é€”**: ä½ç›¸ä¼¼åº¦æ–‡ç« çš„å·®å¼‚åŒ–å»ºè®®
**é€‚ç”¨åœºæ™¯**: ç›¸ä¼¼åº¦<0.5çš„æ–‡ç« ä¼˜åŒ–æŒ‡å¯¼

**æ–‡ä»¶æ•°é‡**: {len(differentiation_suggestions)} ä¸ªå·®å¼‚åŒ–å»ºè®®

## ğŸ”§ å®æ–½æ­¥éª¤

1. **å¤‡ä»½ç½‘ç«™**: åœ¨å®æ–½ä»»ä½•æ›´æ”¹å‰ï¼Œç¡®ä¿å¤‡ä»½æ‚¨çš„ç½‘ç«™
2. **301é‡å®šå‘**: æ ¹æ®redirects.yamlé…ç½®æ‚¨çš„é‡å®šå‘è§„åˆ™
3. **Canonicalæ ‡ç­¾**: æ ¹æ®canonical_mappings.jsonä¸ºç›¸å…³æ–‡ç« æ·»åŠ canonicalæ ‡ç­¾
4. **å†…å®¹ä¼˜åŒ–**: å‚è€ƒå·®å¼‚åŒ–å»ºè®®æ”¹è¿›æ–‡ç« å†…å®¹
5. **æµ‹è¯•éªŒè¯**: å®æ–½åä½¿ç”¨å·¥å…·éªŒè¯é‡å®šå‘å’Œcanonicalæ ‡ç­¾æ˜¯å¦æ­£ç¡®

## âš ï¸ æ³¨æ„äº‹é¡¹

- 301é‡å®šå‘ä¼šæ°¸ä¹…æ”¹å˜URLè®¿é—®ï¼Œè¯·è°¨æ…æ“ä½œ
- å®æ–½å‰è¯·ç¡®è®¤é‡å®šå‘ç›®æ ‡æ–‡ç« ç¡®å®æ˜¯æ‚¨æƒ³è¦ä¿ç•™çš„ç‰ˆæœ¬
- canonicalæ ‡ç­¾ä¸ä¼šå½±å“ç”¨æˆ·è®¿é—®ï¼Œä½†ä¼šå½±å“æœç´¢å¼•æ“ç´¢å¼•
- å»ºè®®åˆ†æ‰¹å®æ–½ï¼Œå…ˆæµ‹è¯•å°‘é‡æ–‡ç« çš„æ•ˆæœ

---
*é…ç½®æ–‡ä»¶ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*è´¨é‡æ£€æµ‹å·¥å…·ç‰ˆæœ¬: v2.3*
"""

        readme_file = os.path.join(output_dir, 'SEO_CONFIG_README.md')
        try:
            with open(readme_file, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            config_files['readme'] = readme_file
            print(f"âœ… å®æ–½è¯´æ˜æ–‡æ¡£å·²ç”Ÿæˆ: {readme_file}")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¯´æ˜æ–‡æ¡£å¤±è´¥: {e}")

        print(f"ğŸ“Š é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(config_files)} ä¸ªæ–‡ä»¶")
        return config_files

    def generate_simple_report(self, detection_result: Dict[str, Any], output_file: str = "simple_report.md") -> str:
        """ç”Ÿæˆç®€åŒ–ç‰ˆæ£€æµ‹æŠ¥å‘Š - é€‚åˆå¿«é€Ÿæ£€æŸ¥

        Args:
            detection_result: æ£€æµ‹ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ“„ æ­£åœ¨ç”Ÿæˆç®€åŒ–æŠ¥å‘Š...")

        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        duplicate_groups = detection_result.get('duplicate_groups', [])
        unique_articles = detection_result.get('unique_articles', [])

        # ç»Ÿè®¡ä¿¡æ¯
        total_duplicates = sum(len(group['articles']) for group in duplicate_groups)
        total_articles = total_duplicates + len(unique_articles)

        report_parts = []

        # ç®€åŒ–æŠ¥å‘Šå¤´éƒ¨
        report_parts.append(f"""# ğŸ“Š ç›¸ä¼¼åº¦æ£€æµ‹ç»“æœ (ç®€åŒ–ç‰ˆ)

**æ£€æµ‹æ—¶é—´**: {timestamp}
**æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold}
**æ–‡ç« æ€»æ•°**: {total_articles} ç¯‡
**é‡å¤ç¾¤ç»„**: {len(duplicate_groups)} ä¸ª
**é‡å¤æ–‡ç« **: {total_duplicates} ç¯‡
**ç‹¬ç«‹æ–‡ç« **: {len(unique_articles)} ç¯‡

---""")

        # æ ¸å¿ƒå‘ç°
        if duplicate_groups:
            # è®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦
            max_similarity = 0.0
            for group in duplicate_groups:
                non_base_articles = [a for a in group['articles'] if not a['is_base']]
                if non_base_articles:
                    group_max = max(a['similarity_to_base'] for a in non_base_articles)
                    max_similarity = max(max_similarity, group_max)

            report_parts.append(f"""## âš ï¸ å‘ç°é—®é¢˜

ğŸ” **æ£€æµ‹åˆ° {len(duplicate_groups)} ä¸ªé‡å¤ç¾¤ç»„**ï¼Œæœ€é«˜ç›¸ä¼¼åº¦ {max_similarity:.3f}

### ğŸ“‹ å¤„ç†å»ºè®®

""")

            # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„å¤„ç†å»ºè®®
            high_similarity_groups = []
            medium_similarity_groups = []
            low_similarity_groups = []

            for group in duplicate_groups:
                non_base_articles = [a for a in group['articles'] if not a['is_base']]
                if non_base_articles:
                    max_sim = max(a['similarity_to_base'] for a in non_base_articles)
                    if max_sim >= 0.8:
                        high_similarity_groups.append(group)
                    elif max_sim >= 0.5:
                        medium_similarity_groups.append(group)
                    else:
                        low_similarity_groups.append(group)

            if high_similarity_groups:
                report_parts.append(f"""**ğŸ”´ é«˜ç›¸ä¼¼åº¦ç¾¤ç»„ ({len(high_similarity_groups)}ä¸ª) - å»ºè®®301é‡å®šå‘**
- å†…å®¹é‡å¤åº¦é«˜ï¼Œå»ºè®®åˆå¹¶æ–‡ç« å¹¶è®¾ç½®301é‡å®šå‘
- ä¿ç•™åŸºå‡†æ–‡ç« ï¼Œåˆ é™¤é‡å¤ç‰ˆæœ¬
""")

            if medium_similarity_groups:
                report_parts.append(f"""**ğŸŸ¡ ä¸­ç­‰ç›¸ä¼¼åº¦ç¾¤ç»„ ({len(medium_similarity_groups)}ä¸ª) - å»ºè®®Canonicalæ ‡ç­¾**
- å†…å®¹éƒ¨åˆ†é‡å¤ï¼Œå»ºè®®ä¿ç•™ä½†è®¾ç½®canonicalæ ‡ç­¾
- å‘ŠçŸ¥æœç´¢å¼•æ“ä»¥åŸºå‡†æ–‡ç« ä¸ºå‡†
""")

            if low_similarity_groups:
                report_parts.append(f"""**ğŸŸ¢ ä½ç›¸ä¼¼åº¦ç¾¤ç»„ ({len(low_similarity_groups)}ä¸ª) - å»ºè®®å†…å®¹å·®å¼‚åŒ–**
- ç›¸ä¼¼åº¦è¾ƒä½ï¼Œé€šè¿‡å†…å®¹ä¼˜åŒ–å¯å®ç°å·®å¼‚åŒ–
- ä¸ºå„æ–‡ç« è¡¥å……ä¸åŒçš„è§‚ç‚¹ã€æ¡ˆä¾‹æˆ–æ•°æ®
""")

            # é‡å¤ç¾¤ç»„ç®€è¦åˆ—è¡¨
            report_parts.append("\n### ğŸ“¦ é‡å¤ç¾¤ç»„è¯¦æƒ…\n")
            for i, group in enumerate(duplicate_groups, 1):
                base_article = group['base_article']
                non_base_count = len([a for a in group['articles'] if not a['is_base']])
                max_sim = max((a['similarity_to_base'] for a in group['articles'] if not a['is_base']), default=0.0)

                status_icon = "ğŸ”´" if max_sim >= 0.8 else "ğŸŸ¡" if max_sim >= 0.5 else "ğŸŸ¢"

                report_parts.append(f"""**{status_icon} ç¾¤ç»„ {i}** (ç›¸ä¼¼åº¦: {max_sim:.3f})
- **ä¿ç•™**: `{base_article['file_name']}`
- **é‡å¤**: {non_base_count} ç¯‡æ–‡ç« 
""")

        else:
            report_parts.append("""## âœ… æ— é‡å¤å†…å®¹

æ­å–œï¼æœªå‘ç°é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡ç« ã€‚æ‚¨çš„å†…å®¹å…·æœ‰è‰¯å¥½çš„ç‹¬ç‰¹æ€§ã€‚

""")

        # ç®€åŒ–æŠ€æœ¯è¯´æ˜
        report_parts.append(f"""---

## ğŸ“ è¯´æ˜

- **æ£€æµ‹ç®—æ³•**: å…¨è¿æ¥å›¾èšç±» (ç¡®ä¿æ•°å­¦å®Œæ•´æ€§)
- **æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold} (ç›¸ä¼¼åº¦â‰¥æ­¤å€¼è¢«è®¤ä¸ºé‡å¤)
- **æ—¶é—´çª—å£**: {self.comparison_window_days} å¤©
- **å­—æ•°é—¨æ§›**: {self.config.get('min_content_length', 1000)} å­—

*ç”Ÿæˆæ—¶é—´: {timestamp} | å·¥å…·ç‰ˆæœ¬: v2.3*""")

        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        report_content = '\n'.join(report_parts)

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            print(f"âœ… ç®€åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            return output_file

        except Exception as e:
            print(f"âŒ ç®€åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‹¬ç«‹æ–‡ç« ç›¸ä¼¼åº¦æ£€æµ‹å·¥å…·')
    parser.add_argument('directory', nargs='?', help='è¦æ£€æµ‹çš„æ–‡ç« ç›®å½•è·¯å¾„')
    parser.add_argument('--config', default='../hugo_quality_standards.yml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--threshold', type=float, help='ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)')
    parser.add_argument('--output', default='similarity_report.md', help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--auto-process', action='store_true',
                       help='è‡ªåŠ¨ç§»åŠ¨é‡å¤æ–‡ç« åˆ°duplicate_articlesæ–‡ä»¶å¤¹')
    parser.add_argument('--dry-run', action='store_true',
                       help='åªæ£€æµ‹ä¸ç§»åŠ¨æ–‡ä»¶ï¼ˆé¢„è§ˆæ¨¡å¼ï¼‰')
    parser.add_argument('--window-days', type=int,
                       help='æ£€æµ‹æ—¶é—´çª—å£ï¼ˆå¤©æ•°ï¼‰')
    parser.add_argument('--compare', nargs=2, metavar=('FILE1', 'FILE2'),
                       help='å¯¹æ¯”ä¸¤ç¯‡æŒ‡å®šæ–‡ç« çš„ç›¸ä¼¼åº¦')
    parser.add_argument('--debug', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†çš„æ¯”è¾ƒè¿‡ç¨‹å’Œè°ƒè¯•ä¿¡æ¯')
    parser.add_argument('--algorithm', choices=['linear', 'graph'], default='linear',
                       help='é€‰æ‹©æ£€æµ‹ç®—æ³•: linear(çº¿æ€§,é»˜è®¤) æˆ– graph(å…¨è¿æ¥å›¾)')
    parser.add_argument('--generate-config', action='store_true',
                       help='ç”ŸæˆSEOä¼˜åŒ–é…ç½®æ–‡ä»¶ (redirects.yaml, canonical_mappings.jsonç­‰)')
    parser.add_argument('--config-output-dir', default='.',
                       help='é…ç½®æ–‡ä»¶è¾“å‡ºç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)')
    parser.add_argument('--simple', action='store_true',
                       help='ç®€åŒ–æ¨¡å¼ï¼šè¾“å‡ºç²¾ç®€çš„ç»“æœæ‘˜è¦ï¼Œé€‚åˆå¿«é€Ÿæ£€æŸ¥')

    args = parser.parse_args()

    if args.simple:
        print("ğŸš€ ç›¸ä¼¼åº¦æ£€æµ‹å·¥å…· (ç®€åŒ–æ¨¡å¼)")
    else:
        print("ğŸš€ ç‹¬ç«‹æ–‡ç« ç›¸ä¼¼åº¦æ£€æµ‹å·¥å…·")
        print("=" * 50)

    # å¤„ç†compareæ¨¡å¼
    if args.compare:
        file1, file2 = args.compare
        print(f"ğŸ” å¯¹æ¯”æ¨¡å¼å¯åŠ¨")

        # åˆå§‹åŒ–æ£€æµ‹å™¨
        checker = SimilarityChecker(args.config)

        # è®¾ç½®è°ƒè¯•æ¨¡å¼
        if args.debug:
            checker.debug_mode = True
            print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

        # æ‰§è¡Œå¯¹æ¯”
        result = checker.compare_two_articles(file1, file2)

        if result:
            print("\nâœ… å¯¹æ¯”å®Œæˆ")
        else:
            print("\nâŒ å¯¹æ¯”å¤±è´¥")
            return 1

        return 0

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not args.directory:
        print(f"âŒ è¯·æŒ‡å®šè¦æ£€æµ‹çš„ç›®å½•è·¯å¾„")
        return 1

    if not Path(args.directory).exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.directory}")
        return 1

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    checker = SimilarityChecker(args.config)

    # è®¾ç½®è°ƒè¯•æ¨¡å¼
    if args.debug:
        checker.debug_mode = True
        print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    # è¦†ç›–é…ç½®å‚æ•°
    if args.threshold:
        checker.similarity_threshold = args.threshold
        print(f"ğŸ“Š ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼: {args.threshold}")

    if args.window_days:
        checker.comparison_window_days = args.window_days
        print(f"â° ä½¿ç”¨è‡ªå®šä¹‰æ—¶é—´çª—å£: {args.window_days} å¤©")

    try:
        # 1. æ‰«ææ–‡ç« 
        if args.simple:
            print(f"ğŸ“ æ‰«æ: {args.directory}")
        else:
            print(f"\nğŸ“ æ‰«æç›®å½•: {args.directory}")
        articles = checker.scan_articles(args.directory)

        if not articles:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ–‡ç« ")
            return 1

        # 2. ç›¸ä¼¼åº¦æ£€æµ‹ (æ ¹æ®ç®—æ³•é€‰æ‹©)
        if args.algorithm == 'graph':
            if args.simple:
                print(f"ğŸ” æ£€æµ‹ç›¸ä¼¼åº¦...")
            else:
                print(f"\nğŸ” å¼€å§‹å…¨è¿æ¥å›¾ç›¸ä¼¼åº¦æ£€æµ‹...")
            detection_result = checker.detect_duplicate_groups(articles)
        else:
            if args.simple:
                print(f"ğŸ” æ£€æµ‹ç›¸ä¼¼åº¦...")
            else:
                print(f"\nğŸ” å¼€å§‹çº¿æ€§ç›¸ä¼¼åº¦æ£€æµ‹...")
            detection_result = checker.detect_similarities(articles)

        # 3. å¤„ç†æ–‡ç« åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
        if args.auto_process and detection_result['moved_articles']:
            print(f"\nğŸ”„ è‡ªåŠ¨å¤„ç†ç›¸ä¼¼æ–‡ç« ...")
            move_files = not args.dry_run
            if args.dry_run:
                print("âš ï¸ é¢„è§ˆæ¨¡å¼ï¼šåªæ˜¾ç¤ºæ“ä½œï¼Œä¸å®é™…ç§»åŠ¨æ–‡ä»¶")

            process_result = checker.process_articles_by_date(detection_result, move_files)

            if move_files:
                print(f"\nâœ… è‡ªåŠ¨å¤„ç†å®Œæˆ:")
                print(f"  âœ… ä¿ç•™æ–‡ç« : {process_result['kept_count']} ç¯‡")
                print(f"  ğŸ“¦ ç§»åŠ¨æ–‡ç« : {process_result['moved_count']} ç¯‡")
                print(f"  ğŸ“ ä¿ç•™æ–‡ç« ç›®å½•: {process_result['new_articles_folder']}")
                print(f"  ğŸ“ ç›¸ä¼¼æ–‡ç« ç›®å½•: {process_result['old_articles_folder']}")
            else:
                print(f"\nğŸ“‹ é¢„è§ˆç»“æœ:")
                print(f"  âœ… å°†ä¿ç•™: {process_result['kept_count']} ç¯‡æ–‡ç« ")
                print(f"  ğŸ“¦ å°†ç§»åŠ¨: {process_result['moved_count']} ç¯‡æ–‡ç« ")

        # 4. ç”ŸæˆæŠ¥å‘Šå’Œæ˜¾ç¤ºæ€»ç»“
        if not args.simple:
            print(f"\n" + "=" * 50)

        if args.algorithm == 'graph':
            duplicate_groups = detection_result.get('duplicate_groups', [])
            unique_articles = detection_result.get('unique_articles', [])
            total_duplicates = sum(len(group['articles']) for group in duplicate_groups)

            if args.simple:
                # ç®€åŒ–æ¨¡å¼è¾“å‡º
                print(f"ğŸ‰ æ£€æµ‹å®Œæˆï¼")
                if duplicate_groups:
                    print(f"âš ï¸  å‘ç°é—®é¢˜: {len(duplicate_groups)} ä¸ªé‡å¤ç¾¤ç»„ï¼Œ{total_duplicates} ç¯‡é‡å¤æ–‡ç« ")
                else:
                    print(f"âœ… æœªå‘ç°é‡å¤å†…å®¹ï¼{len(unique_articles)} ç¯‡æ–‡ç« éƒ½æ˜¯ç‹¬ç«‹åŸåˆ›")
            else:
                # è¯¦ç»†æ¨¡å¼è¾“å‡º
                print(f"ğŸ‰ å…¨è¿æ¥å›¾æ£€æµ‹å®Œæˆï¼")
                print(f"ğŸ“Š æ£€æµ‹äº† {len(articles)} ç¯‡æ–‡ç« ")
                print(f"ğŸ“Š æ€»æ¯”è¾ƒæ¬¡æ•°: {detection_result['total_comparisons']}")
                print(f"ğŸ“¦ å‘ç°é‡å¤ç¾¤ç»„: {len(duplicate_groups)} ä¸ª")
                print(f"ğŸ”¸ é‡å¤æ–‡ç« æ€»æ•°: {total_duplicates} ç¯‡")
                print(f"âœ… ç‹¬ç«‹æ–‡ç« : {len(unique_articles)} ç¯‡")

            # ç”ŸæˆæŠ¥å‘Š (æ ¹æ®æ¨¡å¼é€‰æ‹©)
            if args.simple:
                print(f"\nğŸ“„ ç”Ÿæˆç®€åŒ–æŠ¥å‘Š...")
                report_file = args.output.replace('.md', '_simple.md')
                checker.generate_simple_report(detection_result, report_file)
            else:
                print(f"\nğŸ“„ ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
                report_file = args.output.replace('.md', '_graph_analysis.md')
                checker.generate_duplicate_analysis_report(detection_result, report_file)

            # ç”ŸæˆSEOé…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            if args.generate_config and duplicate_groups:
                print(f"\nğŸ“ ç”ŸæˆSEOä¼˜åŒ–é…ç½®æ–‡ä»¶...")
                config_files = checker.generate_seo_config_files(detection_result, args.config_output_dir)
                if config_files:
                    print(f"ğŸ“Š é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
                    for config_type, file_path in config_files.items():
                        print(f"  âœ… {config_type}: {file_path}")
                else:
                    print("âš ï¸ æœªå‘ç°éœ€è¦ç”Ÿæˆé…ç½®æ–‡ä»¶çš„é‡å¤ç¾¤ç»„")
            elif args.generate_config:
                print("â„¹ï¸ æœªå‘ç°é‡å¤ç¾¤ç»„ï¼Œè·³è¿‡é…ç½®æ–‡ä»¶ç”Ÿæˆ")

        else:
            # çº¿æ€§ç®—æ³•ç»“æœ
            print(f"ğŸ‰ çº¿æ€§æ£€æµ‹å®Œæˆï¼")
            print(f"ğŸ“Š æ£€æµ‹äº† {len(articles)} ç¯‡æ–‡ç« ")
            print(f"ğŸ“Š æ€»æ¯”è¾ƒæ¬¡æ•°: {detection_result['total_comparisons']}")
            print(f"âœ… ä¿ç•™æ–‡ç« : {len(detection_result['kept_articles'])} ç¯‡")
            print(f"ğŸ“¦ ç›¸ä¼¼æ–‡ç« : {len(detection_result['moved_articles'])} ç¯‡")

            if not args.auto_process and detection_result['moved_articles']:
                print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ --auto-process å‚æ•°å¯è‡ªåŠ¨ç§»åŠ¨ç›¸ä¼¼æ–‡ç« ")
                print(f"     ä½¿ç”¨ --dry-run å‚æ•°å¯é¢„è§ˆæ“ä½œè€Œä¸å®é™…ç§»åŠ¨æ–‡ä»¶")

        return 0

    except Exception as e:
        print(f"âŒ æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())