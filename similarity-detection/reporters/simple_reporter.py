#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple Reporter

Generates simplified, concise reports for quick similarity detection overview.
"""

from datetime import datetime
from typing import Dict, List, Any


class SimpleReporter:
    """
    Generates simplified reports for quick review of similarity detection results.
    """

    def __init__(self, config: Dict):
        """
        Initialize simple reporter.

        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.similarity_threshold = config.get('similarity_threshold', 0.7)
        self.comparison_window_days = config.get('comparison_window_days', 90)

    def generate_simple_report(self, detection_result: Dict[str, Any],
                              output_file: str = "simple_report.md") -> str:
        """
        Generate simplified detection report.

        Args:
            detection_result: Detection results
            output_file: Output file path

        Returns:
            Report file path
        """
        print(f"ðŸ“„ æ­£åœ¨ç”Ÿæˆç®€åŒ–æŠ¥å‘Š...")

        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        duplicate_groups = detection_result.get('duplicate_groups', [])
        unique_articles = detection_result.get('unique_articles', [])

        # Statistics
        total_duplicates = sum(len(group['articles']) for group in duplicate_groups)
        total_articles = total_duplicates + len(unique_articles)

        report_parts = []

        # Simplified header
        report_parts.append(f"""# ðŸ“Š ç›¸ä¼¼åº¦æ£€æµ‹ç»“æžœ (ç®€åŒ–ç‰ˆ)

**æ£€æµ‹æ—¶é—´**: {timestamp}
**æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold}
**æ–‡ç« æ€»æ•°**: {total_articles} ç¯‡
**é‡å¤ç¾¤ç»„**: {len(duplicate_groups)} ä¸ª
**é‡å¤æ–‡ç« **: {total_duplicates} ç¯‡
**ç‹¬ç«‹æ–‡ç« **: {len(unique_articles)} ç¯‡

---""")

        # Core findings
        if duplicate_groups:
            # Calculate highest similarity
            max_similarity = 0.0
            for group in duplicate_groups:
                non_base_articles = [a for a in group['articles'] if not a.get('is_base', False)]
                if non_base_articles:
                    group_max = max(a['similarity_to_base'] for a in non_base_articles)
                    max_similarity = max(max_similarity, group_max)

            report_parts.append(f"""## âš ï¸ å‘çŽ°é—®é¢˜

ðŸ” **æ£€æµ‹åˆ° {len(duplicate_groups)} ä¸ªé‡å¤ç¾¤ç»„**ï¼Œæœ€é«˜ç›¸ä¼¼åº¦ {max_similarity:.3f}

### ðŸ“‹ å¤„ç†å»ºè®®

""")

            # Group recommendations by similarity level
            high_similarity_groups = []
            medium_similarity_groups = []
            low_similarity_groups = []

            for group in duplicate_groups:
                non_base_articles = [a for a in group['articles'] if not a.get('is_base', False)]
                if non_base_articles:
                    max_sim = max(a['similarity_to_base'] for a in non_base_articles)
                    if max_sim >= 0.8:
                        high_similarity_groups.append(group)
                    elif max_sim >= 0.5:
                        medium_similarity_groups.append(group)
                    else:
                        low_similarity_groups.append(group)

            if high_similarity_groups:
                report_parts.append(f"""**ðŸ”´ é«˜ç›¸ä¼¼åº¦ç¾¤ç»„ ({len(high_similarity_groups)}ä¸ª) - å»ºè®®301é‡å®šå‘**
- å†…å®¹é‡å¤åº¦é«˜ï¼Œå»ºè®®åˆå¹¶æ–‡ç« å¹¶è®¾ç½®301é‡å®šå‘
- ä¿ç•™åŸºå‡†æ–‡ç« ï¼Œåˆ é™¤é‡å¤ç‰ˆæœ¬
""")

            if medium_similarity_groups:
                report_parts.append(f"""**ðŸŸ¡ ä¸­ç­‰ç›¸ä¼¼åº¦ç¾¤ç»„ ({len(medium_similarity_groups)}ä¸ª) - å»ºè®®Canonicalæ ‡ç­¾**
- å†…å®¹éƒ¨åˆ†é‡å¤ï¼Œå»ºè®®ä¿ç•™ä½†è®¾ç½®canonicalæ ‡ç­¾
- å‘ŠçŸ¥æœç´¢å¼•æ“Žä»¥åŸºå‡†æ–‡ç« ä¸ºå‡†
""")

            if low_similarity_groups:
                report_parts.append(f"""**ðŸŸ¢ ä½Žç›¸ä¼¼åº¦ç¾¤ç»„ ({len(low_similarity_groups)}ä¸ª) - å»ºè®®å†…å®¹å·®å¼‚åŒ–**
- ç›¸ä¼¼åº¦è¾ƒä½Žï¼Œé€šè¿‡å†…å®¹ä¼˜åŒ–å¯å®žçŽ°å·®å¼‚åŒ–
- ä¸ºå„æ–‡ç« è¡¥å……ä¸åŒçš„è§‚ç‚¹ã€æ¡ˆä¾‹æˆ–æ•°æ®
""")

            # Brief group details
            report_parts.append("\n### ðŸ“¦ é‡å¤ç¾¤ç»„è¯¦æƒ…\n")
            for i, group in enumerate(duplicate_groups, 1):
                base_article = group['base_article']
                non_base_count = len([a for a in group['articles'] if not a.get('is_base', False)])
                max_sim = max((a['similarity_to_base'] for a in group['articles'] if not a.get('is_base', False)), default=0.0)

                status_icon = "ðŸ”´" if max_sim >= 0.8 else "ðŸŸ¡" if max_sim >= 0.5 else "ðŸŸ¢"

                report_parts.append(f"""**{status_icon} ç¾¤ç»„ {i}** (ç›¸ä¼¼åº¦: {max_sim:.3f})
- **ä¿ç•™**: `{base_article['file_name']}`
- **é‡å¤**: {non_base_count} ç¯‡æ–‡ç« 
""")

        else:
            report_parts.append("""## âœ… æ— é‡å¤å†…å®¹

æ­å–œï¼æœªå‘çŽ°é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡ç« ã€‚æ‚¨çš„å†…å®¹å…·æœ‰è‰¯å¥½çš„ç‹¬ç‰¹æ€§ã€‚

""")

        # Simplified technical info
        report_parts.append(f"""---

## ðŸ“ è¯´æ˜Ž

- **æ£€æµ‹ç®—æ³•**: å…¨è¿žæŽ¥å›¾èšç±» (ç¡®ä¿æ•°å­¦å®Œæ•´æ€§)
- **æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold} (ç›¸ä¼¼åº¦â‰¥æ­¤å€¼è¢«è®¤ä¸ºé‡å¤)
- **æ—¶é—´çª—å£**: {self.comparison_window_days} å¤©
- **å­—æ•°é—¨æ§›**: {self.config.get('min_content_length', 1000)} å­—

*ç”Ÿæˆæ—¶é—´: {timestamp} | å·¥å…·ç‰ˆæœ¬: v2.3*""")

        # Generate report file
        report_content = '\n'.join(report_parts)

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            print(f"âœ… ç®€åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            return output_file

        except Exception as e:
            print(f"âŒ ç®€åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def generate_quick_summary(self, detection_result: Dict[str, Any]) -> str:
        """
        Generate a very brief summary for console output.

        Args:
            detection_result: Detection results

        Returns:
            Summary string
        """
        duplicate_groups = detection_result.get('duplicate_groups', [])
        unique_articles = detection_result.get('unique_articles', [])
        total_comparisons = detection_result.get('total_comparisons', 0)

        total_duplicates = sum(len(group['articles']) for group in duplicate_groups)
        total_articles = total_duplicates + len(unique_articles)

        if duplicate_groups:
            # Calculate severity
            high_similarity_count = 0
            for group in duplicate_groups:
                non_base_articles = [a for a in group['articles'] if not a.get('is_base', False)]
                if non_base_articles:
                    max_sim = max(a['similarity_to_base'] for a in non_base_articles)
                    if max_sim >= 0.8:
                        high_similarity_count += 1

            severity = "ðŸ”´ é«˜" if high_similarity_count > 0 else "ðŸŸ¡ ä¸­ç­‰"

            summary = f"""ðŸ“Š æ£€æµ‹å®Œæˆï¼å‘çŽ° {len(duplicate_groups)} ä¸ªé‡å¤ç¾¤ç»„
   âš ï¸ é£Žé™©ç­‰çº§: {severity}
   ðŸ“„ æ€»æ–‡ç« : {total_articles} ç¯‡ | é‡å¤: {total_duplicates} ç¯‡ | ç‹¬ç«‹: {len(unique_articles)} ç¯‡
   ðŸ” æ¯”è¾ƒæ¬¡æ•°: {total_comparisons}"""
        else:
            summary = f"""âœ… æ£€æµ‹å®Œæˆï¼æœªå‘çŽ°é‡å¤å†…å®¹
   ðŸ“„ æ£€æµ‹æ–‡ç« : {total_articles} ç¯‡ (å…¨éƒ¨ç‹¬ç«‹)
   ðŸ” æ¯”è¾ƒæ¬¡æ•°: {total_comparisons}"""

        return summary

    def generate_console_output(self, detection_result: Dict[str, Any]) -> None:
        """
        Generate formatted console output for detection results.

        Args:
            detection_result: Detection results
        """
        print("\n" + "="*60)
        print("ðŸŽ‰ ç›¸ä¼¼åº¦æ£€æµ‹ç»“æžœæ‘˜è¦")
        print("="*60)

        summary = self.generate_quick_summary(detection_result)
        print(summary)

        duplicate_groups = detection_result.get('duplicate_groups', [])
        if duplicate_groups:
            print(f"\nðŸ“‹ éœ€è¦å¤„ç†çš„é‡å¤ç¾¤ç»„:")

            for i, group in enumerate(duplicate_groups[:5], 1):  # Show first 5 groups
                base_article = group['base_article']
                non_base_count = len([a for a in group['articles'] if not a.get('is_base', False)])
                max_sim = max((a['similarity_to_base'] for a in group['articles'] if not a.get('is_base', False)), default=0.0)

                status = "ðŸ”´ é«˜é£Žé™©" if max_sim >= 0.8 else "ðŸŸ¡ ä¸­ç­‰é£Žé™©" if max_sim >= 0.5 else "ðŸŸ¢ ä½Žé£Žé™©"
                print(f"   {i}. {status} | åŸºå‡†: {base_article['file_name']} | é‡å¤: {non_base_count} ç¯‡ | ç›¸ä¼¼åº¦: {max_sim:.3f}")

            if len(duplicate_groups) > 5:
                print(f"   ... ä»¥åŠå…¶ä»– {len(duplicate_groups) - 5} ä¸ªç¾¤ç»„")

            print(f"\nðŸ’¡ å»ºè®®æŸ¥çœ‹è¯¦ç»†æŠ¥å‘ŠèŽ·å–å®Œæ•´çš„å¤„ç†æŒ‡å¯¼")

        print("="*60)

    def generate_statistics_only(self, detection_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate statistics summary without full report.

        Args:
            detection_result: Detection results

        Returns:
            Statistics dictionary
        """
        duplicate_groups = detection_result.get('duplicate_groups', [])
        unique_articles = detection_result.get('unique_articles', [])
        total_comparisons = detection_result.get('total_comparisons', 0)

        total_duplicates = sum(len(group['articles']) for group in duplicate_groups)
        total_articles = total_duplicates + len(unique_articles)

        # Calculate similarity distribution
        similarity_distribution = {'high': 0, 'medium': 0, 'low': 0}
        max_overall_similarity = 0.0

        for group in duplicate_groups:
            non_base_articles = [a for a in group['articles'] if not a.get('is_base', False)]
            if non_base_articles:
                max_sim = max(a['similarity_to_base'] for a in non_base_articles)
                max_overall_similarity = max(max_overall_similarity, max_sim)

                if max_sim >= 0.8:
                    similarity_distribution['high'] += 1
                elif max_sim >= 0.5:
                    similarity_distribution['medium'] += 1
                else:
                    similarity_distribution['low'] += 1

        return {
            'total_articles': total_articles,
            'unique_articles': len(unique_articles),
            'duplicate_groups': len(duplicate_groups),
            'duplicate_articles': total_duplicates,
            'duplicate_rate': (total_duplicates / total_articles * 100) if total_articles > 0 else 0,
            'total_comparisons': total_comparisons,
            'max_similarity': max_overall_similarity,
            'similarity_distribution': similarity_distribution,
            'detection_threshold': self.similarity_threshold
        }