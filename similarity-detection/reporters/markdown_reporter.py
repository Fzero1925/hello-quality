#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Markdown Reporter

Generates detailed Markdown reports for similarity detection results.
"""

from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path


class MarkdownReporter:
    """
    Generates comprehensive Markdown reports for similarity detection analysis.
    """

    def __init__(self, config: Dict):
        """
        Initialize Markdown reporter.

        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.similarity_threshold = config.get('similarity_threshold', 0.7)
        self.comparison_window_days = config.get('comparison_window_days', 90)

    def generate_duplicate_analysis_report(self, detection_result: Dict[str, Any],
                                         output_file: str = "duplicate_analysis_report.md") -> str:
        """
        Generate detailed analysis report for duplicate groups.

        Args:
            detection_result: Detection results from graph clustering
            output_file: Output file path

        Returns:
            Report file path
        """
        print(f"ğŸ“„ æ­£åœ¨ç”Ÿæˆé‡å¤æ–‡ç« åˆ†ææŠ¥å‘Š...")

        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        duplicate_groups = detection_result.get('duplicate_groups', [])
        unique_articles = detection_result.get('unique_articles', [])
        total_comparisons = detection_result.get('total_comparisons', 0)
        algorithm = detection_result.get('algorithm', 'graph_clustering')

        # Statistics
        total_duplicates = sum(len(group['articles']) for group in duplicate_groups)
        total_articles = total_duplicates + len(unique_articles)

        report_parts = []

        # Report header
        report_parts.append(self._generate_report_header(
            timestamp, algorithm, total_articles, total_comparisons,
            len(duplicate_groups), total_duplicates, len(unique_articles)
        ))

        # Executive summary
        if duplicate_groups:
            report_parts.append(self._generate_executive_summary(
                duplicate_groups, total_duplicates, len(unique_articles), total_articles
            ))

        # Duplicate groups details
        if duplicate_groups:
            report_parts.append(self._generate_duplicate_groups_section(duplicate_groups))

        # SEO optimization suggestions
        if duplicate_groups:
            report_parts.append(self._generate_seo_optimization_section(duplicate_groups))

        # Unique articles section
        if unique_articles:
            report_parts.append(self._generate_unique_articles_section(unique_articles))

        # Technical details
        report_parts.append(self._generate_technical_section())

        # Generate report file
        report_content = '\n'.join(report_parts)

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            print(f"âœ… é‡å¤æ–‡ç« åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            return output_file

        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def _generate_report_header(self, timestamp: str, algorithm: str, total_articles: int,
                               total_comparisons: int, duplicate_groups_count: int,
                               total_duplicates: int, unique_articles_count: int) -> str:
        """Generate report header section."""
        return f"""# ğŸ“Š é‡å¤æ–‡ç« ç¾¤ç»„åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}
**æ£€æµ‹ç®—æ³•**: {algorithm} (å…¨è¿æ¥å›¾èšç±»)
**æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold}
**æ€»æ–‡ç« æ•°**: {total_articles}
**æ¯”è¾ƒæ¬¡æ•°**: {total_comparisons}
**é‡å¤ç¾¤ç»„**: {duplicate_groups_count} ä¸ª
**é‡å¤æ–‡ç« **: {total_duplicates} ç¯‡
**ç‹¬ç«‹æ–‡ç« **: {unique_articles_count} ç¯‡
**é‡å¤ç‡**: {(total_duplicates/total_articles*100):.1f}%

---"""

    def _generate_executive_summary(self, duplicate_groups: List[Dict],
                                   total_duplicates: int, unique_count: int,
                                   total_articles: int) -> str:
        """Generate executive summary section."""
        avg_group_size = total_duplicates / len(duplicate_groups) if duplicate_groups else 0

        return f"""
## ğŸ¯ æ‰§è¡Œæ‘˜è¦

æœ¬æ¬¡æ£€æµ‹ä½¿ç”¨å…¨è¿æ¥å›¾èšç±»ç®—æ³•ï¼Œç¡®ä¿æ•°å­¦å®Œæ•´æ€§ï¼Œæ— é—æ¼ä»»ä½•ç›¸ä¼¼å…³ç³»ã€‚

### å…³é”®å‘ç°
- ğŸ“¦ å‘ç° **{len(duplicate_groups)}** ä¸ªé‡å¤æ–‡ç« ç¾¤ç»„
- ğŸ”¸ æ¶‰åŠ **{total_duplicates}** ç¯‡é‡å¤æ–‡ç« 
- âœ… **{unique_count}** ç¯‡æ–‡ç« å®Œå…¨ç‹¬ç«‹
- ğŸ“ˆ å¹³å‡æ¯ç¾¤ç»„ **{avg_group_size:.1f}** ç¯‡æ–‡ç« 

### ç®—æ³•ä¼˜åŠ¿
- âœ… **æ•°å­¦å®Œæ•´æ€§**: æ‰€æœ‰æ–‡ç« å¯¹éƒ½è¿›è¡Œäº†æ¯”è¾ƒåˆ†æ
- âœ… **ä¼ é€’æ€§å¤„ç†**: Aâ†’Bâ†’Cå…³ç³»è¢«æ­£ç¡®è¯†åˆ«ä¸ºä¸€ä¸ªç¾¤ç»„
- âœ… **é›¶é—æ¼**: è¿é€šåˆ†é‡ç®—æ³•ç¡®ä¿ä¸é—æ¼ä»»ä½•ç›¸ä¼¼å…³ç³»
- âœ… **SEOå‹å¥½**: æ¯ä¸ªç¾¤ç»„ä¿ç•™æœ€æ—©å‘å¸ƒçš„æ–‡ç« ï¼ˆSEOä»·å€¼æœ€é«˜ï¼‰
"""

    def _generate_duplicate_groups_section(self, duplicate_groups: List[Dict]) -> str:
        """Generate duplicate groups details section."""
        sections = ["\n## ğŸ“¦ é‡å¤æ–‡ç« ç¾¤ç»„è¯¦æƒ…\n"]

        for i, group in enumerate(duplicate_groups, 1):
            base_article = group['base_article']
            articles = group['articles']
            topic = group.get('topic', 'Unknown')

            # Group header
            base_date = self._get_article_date(base_article).strftime('%Y-%m-%d')
            min_sim = min((a['similarity_to_base'] for a in articles if not a.get('is_base', False)), default=1.0)
            max_sim = max((a['similarity_to_base'] for a in articles if not a.get('is_base', False)), default=1.0)

            sections.append(f"""### ç¾¤ç»„ {i}: {topic} ä¸»é¢˜ ({len(articles)}ç¯‡æ–‡ç« )

**åŸºå‡†æ–‡ç« **: `{base_article['file_name']}` (å‘å¸ƒæ—¥æœŸ: {base_date})
**ç¾¤ç»„ä¸»é¢˜**: {topic}
**ç›¸ä¼¼åº¦èŒƒå›´**: {min_sim:.3f} - {max_sim:.3f}

#### æ–‡ç« åˆ—è¡¨:""")

            # Article details
            for article in articles:
                if article.get('is_base', False):
                    marker = "ğŸ”¹ **åŸºå‡†**"
                    sim_text = "1.000"
                else:
                    marker = "ğŸ”¸"
                    sim_text = f"{article['similarity_to_base']:.3f}"

                article_date = self._get_article_date(article).strftime('%Y-%m-%d')
                sections.append(f"- {marker} `{article['file_name']}` (ç›¸ä¼¼åº¦: {sim_text}, æ—¥æœŸ: {article_date})")

            sections.append("")  # Empty line separator

        return '\n'.join(sections)

    def _generate_seo_optimization_section(self, duplicate_groups: List[Dict]) -> str:
        """Generate SEO optimization suggestions section."""
        sections = [f"""
## ğŸ”§ SEOä¼˜åŒ–å¤„ç†å»ºè®®

åŸºäºæœ€ä½³å®è·µï¼Œä¸ºå‘ç°çš„{len(duplicate_groups)}ä¸ªé‡å¤ç¾¤ç»„æä¾›å…·ä½“çš„SEOä¼˜åŒ–æ–¹æ¡ˆï¼š

### ğŸ“‹ å¤„ç†ç­–ç•¥æ€»è§ˆ

1. **301é‡å®šå‘ (æ¨è)**: åˆå¹¶å†…å®¹ç›¸ä¼¼åº¦>0.8çš„æ–‡ç« ï¼Œè®¾ç½®æ°¸ä¹…é‡å®šå‘ä¿æŠ¤SEOæƒé‡
2. **Canonicalæ ‡ç­¾**: ä¿ç•™ç›¸ä¼¼åº¦0.5-0.8ä¹‹é—´çš„æ–‡ç« ï¼Œä½¿ç”¨canonicalæ ‡ç­¾æŒ‡å‘ä¸»è¦ç‰ˆæœ¬
3. **å†…å®¹å·®å¼‚åŒ–**: ç›¸ä¼¼åº¦<0.7çš„æ–‡ç« å¯é€šè¿‡å†…å®¹è¡¥å……å®ç°å·®å¼‚åŒ–

---"""]

        # Generate specific SEO recommendations for each group
        for i, group in enumerate(duplicate_groups, 1):
            base_article = group['base_article']
            articles = group['articles']
            topic = group.get('topic', 'Unknown')

            # Calculate group similarity statistics
            non_base_articles = [a for a in articles if not a.get('is_base', False)]
            if non_base_articles:
                max_similarity = max(a['similarity_to_base'] for a in non_base_articles)
                avg_similarity = sum(a['similarity_to_base'] for a in non_base_articles) / len(non_base_articles)
            else:
                max_similarity = avg_similarity = 0.0

            sections.append(f"""
### ç¾¤ç»„ {i} å¤„ç†å»ºè®®: {topic}

**åŸºå‡†æ–‡ç« **: `{base_article['file_name']}`
**ç¾¤ç»„ç›¸ä¼¼åº¦**: å¹³å‡ {avg_similarity:.3f}, æœ€é«˜ {max_similarity:.3f}

#### ğŸ¯ æ¨èç­–ç•¥:""")

            # Generate strategy based on similarity level
            if max_similarity >= 0.8:
                sections.extend(self._generate_301_redirect_strategy(base_article, non_base_articles))
            elif max_similarity >= 0.5:
                sections.extend(self._generate_canonical_strategy(base_article, non_base_articles))
            else:
                sections.extend(self._generate_differentiation_strategy())

            sections.append("")

        return '\n'.join(sections)

    def _generate_301_redirect_strategy(self, base_article: Dict, articles: List[Dict]) -> List[str]:
        """Generate 301 redirect strategy text."""
        strategy = [f"""
**301é‡å®šå‘æ–¹æ¡ˆ** (ç›¸ä¼¼åº¦ â‰¥ 0.8)
- âœ… **ä¿ç•™**: `{base_article['file_name']}` (åŸºå‡†æ–‡ç« )
- ğŸ”„ **é‡å®šå‘åˆ°åŸºå‡†æ–‡ç« **:"""]

        for article in articles:
            if article['similarity_to_base'] >= 0.8:
                strategy.append(f"  - `{article['file_name']}` â†’ `{base_article['file_name']}`")

        strategy.extend([f"""
**æ“ä½œæ­¥éª¤**:
1. å°†é‡å¤æ–‡ç« çš„ä¼˜è´¨å†…å®¹åˆå¹¶åˆ°åŸºå‡†æ–‡ç«  `{base_article['file_name']}`
2. åœ¨ç½‘ç«™é…ç½®ä¸­æ·»åŠ 301é‡å®šå‘è§„åˆ™
3. åˆ é™¤é‡å¤æ–‡ç« æ–‡ä»¶
4. æ›´æ–°å†…éƒ¨é“¾æ¥æŒ‡å‘åŸºå‡†æ–‡ç« """])

        return strategy

    def _generate_canonical_strategy(self, base_article: Dict, articles: List[Dict]) -> List[str]:
        """Generate canonical tag strategy text."""
        strategy = [f"""
**Canonicalæ ‡ç­¾æ–¹æ¡ˆ** (ç›¸ä¼¼åº¦ 0.5-0.8)
- âœ… **ä¸»è¦æ–‡ç« **: `{base_article['file_name']}`
- ğŸ·ï¸ **è®¾ç½®Canonicalæ ‡ç­¾**:"""]

        for article in articles:
            if 0.5 <= article['similarity_to_base'] < 0.8:
                strategy.append(f"  - `{article['file_name']}` â†’ canonicalæŒ‡å‘ `{base_article['file_name']}`")

        strategy.extend([f"""
**HTMLæ ‡ç­¾ç¤ºä¾‹**:
```html
<link rel="canonical" href="/articles/{base_article['file_name'].replace('.md', '.html')}" />
```

**æ“ä½œè¯´æ˜**:
1. ä¿ç•™æ‰€æœ‰æ–‡ç« ï¼Œä¸åˆ é™¤ä»»ä½•å†…å®¹
2. åœ¨æ¬¡è¦æ–‡ç« çš„HTMLå¤´éƒ¨æ·»åŠ canonicalæ ‡ç­¾
3. å‘Šè¯‰æœç´¢å¼•æ“ä»¥åŸºå‡†æ–‡ç« ä¸ºå‡†è¿›è¡Œç´¢å¼•"""])

        return strategy

    def _generate_differentiation_strategy(self) -> List[str]:
        """Generate content differentiation strategy text."""
        return ["""
**å†…å®¹å·®å¼‚åŒ–æ–¹æ¡ˆ** (ç›¸ä¼¼åº¦ < 0.5)
- ğŸ“ **å·®å¼‚åŒ–å¤„ç†**: å„æ–‡ç« ä¿æŒç‹¬ç«‹ï¼Œå¢å¼ºå†…å®¹å·®å¼‚æ€§

**ä¼˜åŒ–å»ºè®®**:
1. ä¸ºæ¯ç¯‡æ–‡ç« è¡¥å……ä¸åŒçš„æ¡ˆä¾‹ã€æ•°æ®æˆ–è§‚ç‚¹
2. è°ƒæ•´æ–‡ç« è§’åº¦ï¼šæŠ€æœ¯å®ç° vs ç”¨æˆ·æŒ‡å— vs äº§å“æ¯”è¾ƒ
3. æ·»åŠ ç‹¬ç‰¹çš„åº”ç”¨åœºæ™¯æˆ–è§£å†³æ–¹æ¡ˆ
4. ç¡®ä¿æ¯ç¯‡æ–‡ç« éƒ½æœ‰æ˜ç¡®çš„ç›®æ ‡å—ä¼—"""]

    def _generate_unique_articles_section(self, unique_articles: List[Dict]) -> str:
        """Generate unique articles section."""
        sections = [f"""
## âœ… ç‹¬ç«‹æ–‡ç« åˆ—è¡¨ ({len(unique_articles)}ç¯‡)

ä»¥ä¸‹æ–‡ç« ä¸å…¶ä»–æ–‡ç« ä¸å­˜åœ¨æ˜¾è‘—ç›¸ä¼¼æ€§ï¼Œä¸ºç‹¬ç«‹åŸåˆ›å†…å®¹ï¼š

"""]

        for article in unique_articles:
            article_date = self._get_article_date(article).strftime('%Y-%m-%d')
            word_count = article.get('word_count', 0)
            sections.append(f"- `{article['file_name']}` (æ—¥æœŸ: {article_date}, å­—æ•°: {word_count})")

        return '\n'.join(sections)

    def _generate_technical_section(self) -> str:
        """Generate technical details section."""
        cross_topic_threshold = getattr(self, 'cross_topic_threshold', 0.7)

        return f"""

---

## ğŸ”¬ æŠ€æœ¯è¯´æ˜

### ç®—æ³•åŸç†
1. **ç›¸ä¼¼åº¦çŸ©é˜µæ„å»º**: è®¡ç®—æ‰€æœ‰æ–‡ç« å¯¹çš„TF-IDFä½™å¼¦ç›¸ä¼¼åº¦
2. **å›¾æ„å»º**: ç›¸ä¼¼åº¦â‰¥{self.similarity_threshold}çš„æ–‡ç« å¯¹å½¢æˆè¿æ¥
3. **è¿é€šåˆ†é‡**: ä½¿ç”¨DFSç®—æ³•æ‰¾å‡ºæ‰€æœ‰è¿é€šåˆ†é‡(é‡å¤ç¾¤ç»„)
4. **åŸºå‡†é€‰æ‹©**: æ¯ä¸ªç¾¤ç»„é€‰æ‹©æœ€æ—©å‘å¸ƒçš„æ–‡ç« ä½œä¸ºåŸºå‡†ä¿ç•™

### ç›¸ä¼¼åº¦è®¡ç®—
- **æ ‡é¢˜æƒé‡**: 30%
- **å†…å®¹æƒé‡**: 70%
- **åŒä¸»é¢˜é˜ˆå€¼**: {self.similarity_threshold}
- **è·¨ä¸»é¢˜é˜ˆå€¼**: {cross_topic_threshold}

### æ—¶é—´çª—å£
- **æ£€æµ‹çª—å£**: {self.comparison_window_days} å¤©
- **å­—æ•°é—¨æ§›**: {self.config.get('min_content_length', 1000)} å­—

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ç®—æ³•ç‰ˆæœ¬: Graph Clustering v2.3*"""

    def _get_article_date(self, article: Dict) -> datetime:
        """Get article effective date."""
        # This is a simplified version - in practice, you'd use DateHelper
        if 'effective_date' in article:
            return article['effective_date']
        elif 'created_time' in article:
            return article['created_time']
        elif 'modified_time' in article:
            return article['modified_time']
        else:
            return datetime.now()

    def generate_comparison_report(self, article1: Dict, article2: Dict,
                                 similarity_result: Dict[str, Any],
                                 output_file: str = "comparison_report.md") -> str:
        """
        Generate report for two-article comparison.

        Args:
            article1: First article information
            article2: Second article information
            similarity_result: Similarity calculation results
            output_file: Output file path

        Returns:
            Report file path
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        report_content = f"""# ğŸ“Š æ–‡ç« ç›¸ä¼¼åº¦å¯¹æ¯”æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}
**æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold}

## ğŸ“ å¯¹æ¯”æ–‡ç« 

### æ–‡ç«  A
- **æ–‡ä»¶**: `{article1.get('file_name', 'Unknown')}`
- **æ ‡é¢˜**: {article1.get('title', 'Unknown')}
- **å­—æ•°**: {article1.get('word_count', 0)}
- **æ—¥æœŸ**: {self._get_article_date(article1).strftime('%Y-%m-%d')}

### æ–‡ç«  B
- **æ–‡ä»¶**: `{article2.get('file_name', 'Unknown')}`
- **æ ‡é¢˜**: {article2.get('title', 'Unknown')}
- **å­—æ•°**: {article2.get('word_count', 0)}
- **æ—¥æœŸ**: {self._get_article_date(article2).strftime('%Y-%m-%d')}

## ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ

- **æ ‡é¢˜ç›¸ä¼¼åº¦**: {similarity_result.get('title_similarity', 0):.3f}
- **å†…å®¹ç›¸ä¼¼åº¦**: {similarity_result.get('content_similarity', 0):.3f}
- **ç»¼åˆç›¸ä¼¼åº¦**: {similarity_result.get('overall_similarity', 0):.3f}
- **æ£€æµ‹é˜ˆå€¼**: {self.similarity_threshold:.3f}

## ğŸ¯ ç»“è®º

ç›¸ä¼¼åº¦è¯„åˆ†: **{similarity_result.get('overall_similarity', 0):.3f}**

{'âœ… **ç›¸ä¼¼**: è¶…è¿‡æ£€æµ‹é˜ˆå€¼ï¼Œè¢«åˆ¤å®šä¸ºç›¸ä¼¼å†…å®¹' if similarity_result.get('is_similar', False) else 'âŒ **ä¸ç›¸ä¼¼**: ä½äºæ£€æµ‹é˜ˆå€¼ï¼Œè¢«åˆ¤å®šä¸ºç‹¬ç«‹å†…å®¹'}

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}*
*æ£€æµ‹å·¥å…·ç‰ˆæœ¬: v2.3*
"""

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            return output_file

        except Exception as e:
            print(f"âŒ å¯¹æ¯”æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""