#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEO Config Generator

Generates SEO optimization configuration files based on similarity detection results.
"""

import os
import json
import yaml
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path


class SEOConfigGenerator:
    """
    Generates SEO optimization configuration files for handling duplicate content.
    """

    def __init__(self, config: Dict):
        """
        Initialize SEO config generator.

        Args:
            config: Configuration dictionary
        """
        self.config = config

    def generate_seo_config_files(self, detection_result: Dict[str, Any],
                                 output_dir: str = ".") -> Dict[str, str]:
        """
        Generate comprehensive SEO optimization configuration files.

        Args:
            detection_result: Detection results
            output_dir: Output directory

        Returns:
            Dictionary mapping config types to file paths
        """
        print(f"ğŸ“ æ­£åœ¨ç”ŸæˆSEOä¼˜åŒ–é…ç½®æ–‡ä»¶...")

        duplicate_groups = detection_result.get('duplicate_groups', [])

        # 301 redirect configuration
        redirects_config = {'redirects': []}

        # Canonical tag mappings
        canonical_mappings = {}

        # Content differentiation suggestions
        differentiation_suggestions = []

        for group in duplicate_groups:
            base_article = group['base_article']
            articles = group['articles']

            # Process non-base articles
            non_base_articles = [a for a in articles if not a.get('is_base', False)]
            if not non_base_articles:
                continue

            for article in non_base_articles:
                similarity = article['similarity_to_base']
                source_path = f"/articles/{article['file_name'].replace('.md', '.html')}"
                target_path = f"/articles/{base_article['file_name'].replace('.md', '.html')}"

                if similarity >= 0.8:
                    # 301 redirect for high similarity
                    redirects_config['redirects'].append({
                        'from': source_path,
                        'to': target_path,
                        'status': 301,
                        'reason': f'é‡å¤å†…å®¹åˆå¹¶ (ç›¸ä¼¼åº¦: {similarity:.3f})',
                        'similarity': similarity
                    })

                elif similarity >= 0.5:
                    # Canonical tag for medium similarity
                    canonical_mappings[article['file_name']] = {
                        'canonical_url': target_path,
                        'canonical_file': base_article['file_name'],
                        'similarity': similarity,
                        'html_tag': f'<link rel="canonical" href="{target_path}" />'
                    }

                else:
                    # Content differentiation for low similarity
                    differentiation_suggestions.append({
                        'file': article['file_name'],
                        'base_file': base_article['file_name'],
                        'similarity': similarity,
                        'suggestions': [
                            'ä¸ºæ–‡ç« è¡¥å……ä¸åŒçš„æ¡ˆä¾‹ã€æ•°æ®æˆ–è§‚ç‚¹',
                            'è°ƒæ•´æ–‡ç« è§’åº¦ï¼šæŠ€æœ¯å®ç° vs ç”¨æˆ·æŒ‡å— vs äº§å“æ¯”è¾ƒ',
                            'æ·»åŠ ç‹¬ç‰¹çš„åº”ç”¨åœºæ™¯æˆ–è§£å†³æ–¹æ¡ˆ',
                            'ç¡®ä¿æ–‡ç« æœ‰æ˜ç¡®çš„ç›®æ ‡å—ä¼—'
                        ]
                    })

        # Generate configuration files
        config_files = {}

        # 1. Generate redirects.yaml
        if redirects_config['redirects']:
            redirects_file = os.path.join(output_dir, 'redirects.yaml')
            try:
                with open(redirects_file, 'w', encoding='utf-8') as f:
                    yaml.dump(redirects_config, f, default_flow_style=False,
                            allow_unicode=True, sort_keys=False)
                config_files['redirects'] = redirects_file
                print(f"âœ… 301é‡å®šå‘é…ç½®å·²ç”Ÿæˆ: {redirects_file}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆredirects.yamlå¤±è´¥: {e}")

        # 2. Generate canonical_mappings.json
        if canonical_mappings:
            canonical_file = os.path.join(output_dir, 'canonical_mappings.json')
            try:
                with open(canonical_file, 'w', encoding='utf-8') as f:
                    json.dump(canonical_mappings, f, indent=2, ensure_ascii=False)
                config_files['canonical'] = canonical_file
                print(f"âœ… Canonicalæ ‡ç­¾é…ç½®å·²ç”Ÿæˆ: {canonical_file}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆcanonical_mappings.jsonå¤±è´¥: {e}")

        # 3. Generate content_differentiation.json
        if differentiation_suggestions:
            diff_file = os.path.join(output_dir, 'content_differentiation.json')
            try:
                with open(diff_file, 'w', encoding='utf-8') as f:
                    json.dump(differentiation_suggestions, f, indent=2, ensure_ascii=False)
                config_files['differentiation'] = diff_file
                print(f"âœ… å†…å®¹å·®å¼‚åŒ–å»ºè®®å·²ç”Ÿæˆ: {diff_file}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆcontent_differentiation.jsonå¤±è´¥: {e}")

        # 4. Generate implementation guide
        readme_content = self._generate_implementation_guide(
            len(redirects_config['redirects']),
            len(canonical_mappings),
            len(differentiation_suggestions)
        )

        readme_file = os.path.join(output_dir, 'SEO_CONFIG_README.md')
        try:
            with open(readme_file, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            config_files['readme'] = readme_file
            print(f"âœ… å®æ–½è¯´æ˜æ–‡æ¡£å·²ç”Ÿæˆ: {readme_file}")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¯´æ˜æ–‡æ¡£å¤±è´¥: {e}")

        print(f"ğŸ“Š é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(config_files)} ä¸ªæ–‡ä»¶")
        return config_files

    def _generate_implementation_guide(self, redirect_count: int,
                                     canonical_count: int,
                                     differentiation_count: int) -> str:
        """Generate implementation guide content."""
        return f"""# SEOä¼˜åŒ–é…ç½®æ–‡ä»¶ä½¿ç”¨è¯´æ˜

æœ¬ç›®å½•åŒ…å«è‡ªåŠ¨ç”Ÿæˆçš„SEOä¼˜åŒ–é…ç½®æ–‡ä»¶ï¼Œå¸®åŠ©æ‚¨å¤„ç†ç½‘ç«™ä¸­çš„é‡å¤å†…å®¹é—®é¢˜ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

### 1. redirects.yaml
**ç”¨é€”**: 301æ°¸ä¹…é‡å®šå‘é…ç½®
**é€‚ç”¨åœºæ™¯**: é«˜ç›¸ä¼¼åº¦æ–‡ç« (â‰¥0.8)çš„åˆå¹¶å¤„ç†

**Hugoå®æ–½æ–¹æ³•**:
```yaml
# åœ¨Hugoé…ç½®æ–‡ä»¶(config.yaml)ä¸­æ·»åŠ :
outputs:
  home: ["HTML", "RSS", "REDIRECTS"]

# å°†redirects.yamlå†…å®¹æ·»åŠ åˆ°static/_redirectsæ–‡ä»¶
```

**æ–‡ä»¶æ•°é‡**: {redirect_count} ä¸ªé‡å®šå‘è§„åˆ™

### 2. canonical_mappings.json
**ç”¨é€”**: Canonicalæ ‡ç­¾é…ç½®æ˜ å°„
**é€‚ç”¨åœºæ™¯**: ä¸­ç­‰ç›¸ä¼¼åº¦æ–‡ç« (0.5-0.8)çš„SEOä¼˜åŒ–

**å®æ–½æ–¹æ³•**:
1. åœ¨æ–‡ç« çš„Front Matterä¸­æ·»åŠ canonicalå­—æ®µ
2. æˆ–åœ¨æ¨¡æ¿æ–‡ä»¶çš„<head>éƒ¨åˆ†ä½¿ç”¨é…ç½®ç”Ÿæˆcanonicalæ ‡ç­¾

**æ–‡ä»¶æ•°é‡**: {canonical_count} ä¸ªcanonicalæ˜ å°„

### 3. content_differentiation.json
**ç”¨é€”**: ä½ç›¸ä¼¼åº¦æ–‡ç« çš„å·®å¼‚åŒ–å»ºè®®
**é€‚ç”¨åœºæ™¯**: ç›¸ä¼¼åº¦<0.5çš„æ–‡ç« ä¼˜åŒ–æŒ‡å¯¼

**æ–‡ä»¶æ•°é‡**: {differentiation_count} ä¸ªå·®å¼‚åŒ–å»ºè®®

## ğŸ”§ å®æ–½æ­¥éª¤

1. **å¤‡ä»½ç½‘ç«™**: åœ¨å®æ–½ä»»ä½•æ›´æ”¹å‰ï¼Œç¡®ä¿å¤‡ä»½æ‚¨çš„ç½‘ç«™
2. **301é‡å®šå‘**: æ ¹æ®redirects.yamlé…ç½®æ‚¨çš„é‡å®šå‘è§„åˆ™
3. **Canonicalæ ‡ç­¾**: æ ¹æ®canonical_mappings.jsonä¸ºç›¸å…³æ–‡ç« æ·»åŠ canonicalæ ‡ç­¾
4. **å†…å®¹ä¼˜åŒ–**: å‚è€ƒå·®å¼‚åŒ–å»ºè®®æ”¹è¿›æ–‡ç« å†…å®¹
5. **æµ‹è¯•éªŒè¯**: å®æ–½åä½¿ç”¨å·¥å…·éªŒè¯é‡å®šå‘å’Œcanonicalæ ‡ç­¾æ˜¯å¦æ­£ç¡®

## âš ï¸ æ³¨æ„äº‹é¡¹

- 301é‡å®šå‘ä¼šæ°¸ä¹…æ”¹å˜URLè®¿é—®ï¼Œè¯·è°¨æ…æ“ä½œ
- å®æ–½å‰è¯·ç¡®è®¤é‡å®šå‘ç›®æ ‡æ–‡ç« ç¡®å®æ˜¯æ‚¨æƒ³è¦ä¿ç•™çš„ç‰ˆæœ¬
- canonicalæ ‡ç­¾ä¸ä¼šå½±å“ç”¨æˆ·è®¿é—®ï¼Œä½†ä¼šå½±å“æœç´¢å¼•æ“ç´¢å¼•
- å»ºè®®åˆ†æ‰¹å®æ–½ï¼Œå…ˆæµ‹è¯•å°‘é‡æ–‡ç« çš„æ•ˆæœ

---
*é…ç½®æ–‡ä»¶ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*è´¨é‡æ£€æµ‹å·¥å…·ç‰ˆæœ¬: v2.3*
"""

    def generate_hugo_redirects(self, detection_result: Dict[str, Any],
                               output_file: str = "_redirects") -> str:
        """
        Generate Hugo-specific redirects file.

        Args:
            detection_result: Detection results
            output_file: Output file path

        Returns:
            Generated file path
        """
        duplicate_groups = detection_result.get('duplicate_groups', [])
        redirects_lines = []

        redirects_lines.append("# Auto-generated redirects for duplicate content")
        redirects_lines.append(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        redirects_lines.append("")

        redirect_count = 0

        for group in duplicate_groups:
            base_article = group['base_article']
            articles = group['articles']

            base_url = f"/articles/{base_article['file_name'].replace('.md', '.html')}"

            for article in articles:
                if article.get('is_base', False):
                    continue

                similarity = article['similarity_to_base']
                if similarity >= 0.8:
                    source_url = f"/articles/{article['file_name'].replace('.md', '.html')}"
                    redirects_lines.append(f"{source_url} {base_url} 301")
                    redirect_count += 1

        if redirect_count > 0:
            redirects_lines.append("")
            redirects_lines.append(f"# Total redirects: {redirect_count}")

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(redirects_lines))

            print(f"âœ… Hugoé‡å®šå‘æ–‡ä»¶å·²ç”Ÿæˆ: {output_file} ({redirect_count} ä¸ªé‡å®šå‘)")
            return output_file

        except Exception as e:
            print(f"âŒ ç”ŸæˆHugoé‡å®šå‘æ–‡ä»¶å¤±è´¥: {e}")
            return ""

    def generate_nginx_redirects(self, detection_result: Dict[str, Any],
                                output_file: str = "nginx_redirects.conf") -> str:
        """
        Generate Nginx redirects configuration.

        Args:
            detection_result: Detection results
            output_file: Output file path

        Returns:
            Generated file path
        """
        duplicate_groups = detection_result.get('duplicate_groups', [])
        config_lines = []

        config_lines.append("# Auto-generated Nginx redirects for duplicate content")
        config_lines.append(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        config_lines.append("# Add these lines to your Nginx server configuration")
        config_lines.append("")

        redirect_count = 0

        for group in duplicate_groups:
            base_article = group['base_article']
            articles = group['articles']

            base_url = f"/articles/{base_article['file_name'].replace('.md', '.html')}"

            for article in articles:
                if article.get('is_base', False):
                    continue

                similarity = article['similarity_to_base']
                if similarity >= 0.8:
                    source_url = f"/articles/{article['file_name'].replace('.md', '.html')}"
                    config_lines.append(f'rewrite ^{source_url}$ {base_url} permanent;')
                    redirect_count += 1

        if redirect_count > 0:
            config_lines.append("")
            config_lines.append(f"# Total redirects: {redirect_count}")

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(config_lines))

            print(f"âœ… Nginxé‡å®šå‘é…ç½®å·²ç”Ÿæˆ: {output_file} ({redirect_count} ä¸ªé‡å®šå‘)")
            return output_file

        except Exception as e:
            print(f"âŒ ç”ŸæˆNginxé‡å®šå‘é…ç½®å¤±è´¥: {e}")
            return ""

    def generate_htaccess_redirects(self, detection_result: Dict[str, Any],
                                   output_file: str = ".htaccess") -> str:
        """
        Generate Apache .htaccess redirects.

        Args:
            detection_result: Detection results
            output_file: Output file path

        Returns:
            Generated file path
        """
        duplicate_groups = detection_result.get('duplicate_groups', [])
        config_lines = []

        config_lines.append("# Auto-generated Apache redirects for duplicate content")
        config_lines.append(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        config_lines.append("")

        redirect_count = 0

        for group in duplicate_groups:
            base_article = group['base_article']
            articles = group['articles']

            base_url = f"/articles/{base_article['file_name'].replace('.md', '.html')}"

            for article in articles:
                if article.get('is_base', False):
                    continue

                similarity = article['similarity_to_base']
                if similarity >= 0.8:
                    source_url = f"/articles/{article['file_name'].replace('.md', '.html')}"
                    config_lines.append(f'Redirect 301 {source_url} {base_url}')
                    redirect_count += 1

        if redirect_count > 0:
            config_lines.append("")
            config_lines.append(f"# Total redirects: {redirect_count}")

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(config_lines))

            print(f"âœ… Apache .htaccessæ–‡ä»¶å·²ç”Ÿæˆ: {output_file} ({redirect_count} ä¸ªé‡å®šå‘)")
            return output_file

        except Exception as e:
            print(f"âŒ ç”Ÿæˆ.htaccessæ–‡ä»¶å¤±è´¥: {e}")
            return ""

    def generate_json_ld_schema(self, detection_result: Dict[str, Any],
                               output_file: str = "seo_schema.json") -> str:
        """
        Generate JSON-LD schema markup for SEO.

        Args:
            detection_result: Detection results
            output_file: Output file path

        Returns:
            Generated file path
        """
        duplicate_groups = detection_result.get('duplicate_groups', [])
        unique_articles = detection_result.get('unique_articles', [])

        # Collect all canonical/primary articles
        primary_articles = []

        # Add base articles from duplicate groups
        for group in duplicate_groups:
            primary_articles.append(group['base_article'])

        # Add unique articles
        primary_articles.extend(unique_articles)

        # Generate schema
        schema_data = {
            "@context": "https://schema.org",
            "@type": "ItemList",
            "name": "Website Articles",
            "description": f"Collection of {len(primary_articles)} unique articles",
            "numberOfItems": len(primary_articles),
            "itemListElement": []
        }

        for i, article in enumerate(primary_articles, 1):
            article_item = {
                "@type": "Article",
                "position": i,
                "headline": article.get('title', 'Untitled'),
                "url": f"/articles/{article['file_name'].replace('.md', '.html')}",
                "wordCount": article.get('word_count', 0),
                "datePublished": article.get('created_time', datetime.now()).isoformat() if isinstance(article.get('created_time'), datetime) else datetime.now().isoformat(),
                "dateModified": article.get('modified_time', datetime.now()).isoformat() if isinstance(article.get('modified_time'), datetime) else datetime.now().isoformat()
            }
            schema_data["itemListElement"].append(article_item)

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(schema_data, f, indent=2, ensure_ascii=False, default=str)

            print(f"âœ… JSON-LD schemaå·²ç”Ÿæˆ: {output_file}")
            return output_file

        except Exception as e:
            print(f"âŒ ç”ŸæˆJSON-LD schemaå¤±è´¥: {e}")
            return ""